{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9737512,"sourceType":"datasetVersion","datasetId":5959883},{"sourceId":9823811,"sourceType":"datasetVersion","datasetId":6024014},{"sourceId":10048316,"sourceType":"datasetVersion","datasetId":6190771},{"sourceId":10052283,"sourceType":"datasetVersion","datasetId":6193773},{"sourceId":182747,"sourceType":"modelInstanceVersion","modelInstanceId":155763,"modelId":178220},{"sourceId":182847,"sourceType":"modelInstanceVersion","modelInstanceId":155852,"modelId":178308},{"sourceId":183225,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":156171,"modelId":178621}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Sơ lược về dataset và model\n* Dataset:\n  * **soict-dataset-2024**: Bộ dữ liêu gốc.\n  * **soict-dataset-2024-segmented**: Bộ dữ liêu gốc được segment dùng underthesea\n  * **cross-encoder-dataset**: Dataset này được tạo ra bằng cách lấy ground truth + 3 hard negative trả về bởi bi-encoder + 1 random negative,tập train gồm hơn 500000 sample và tập val có hơn 100000 sample\n  * **cross-encoder-dataset-segmented**: Giống cross-encoder-dataset nhưng được tạo ra bằng bi-encoder mới và có segmentation\n* Mô hình:\n    * **bi_encoder**: Đây là mô hình BiEncoder với base model là  https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder được train trên tập dữ liệu **soict-dataset-2024-segmented** với 2 epoch có mine hard negative\n    * **bi_encoder_embedding** : Đây là FAISS vector database chứa các embedding vector của toàn bộ document trong corpus_segmented.csv được tạo ra bằng cách dùng document_encoder của của bi_encoder\n    * **cross_encoder_new**: Mô hình cross-encoder được tạo ra bằng thư viện sentence_transformer và được train trên tập dữ liệu **soict-dataset-2024-segmented**, dùng để reranking các kết quả trả về từ bi-encoder \n","metadata":{}},{"cell_type":"markdown","source":"### Những thứ đã được thực hiện trong notebook này:\n1. Train một mô hình bi-encoder với XML-RoBERTA với 1 epoch **bi_encoder_xlmRoBERTA** , chọn mô hình nàu làm baseline vì nó đã được pretrain bằng nhiều ngôn ngữ gồm tiếng Việt\n2. Sử dụng mô hình bi-encoder vừa train để thực hiện retrieval trên toàn bộ dataset, với mỗi sample trong dataset lọc lấy các ground truth, và chọn 3 hard negative từ kết quả trả về (có thể hiểu ứng với mỗi query chọn 3 kết quả sai được rank cao nhất khi thực hiện retrieval bằng bi-encoder) cùng với 1 random negative, tổng hợp lại ta được một dataset mới là **cross-encoder-dataset** dùng để train cross-encoder (lí do phải tạo dataset mới để train cross-encoder là vì cross-encoder sẽ được dùng để reranking các candidate trả về bởi bi-encoder, mà các candidate là top@k trả về từ bi-encoder do đó rất tương đồng với câu query vì thế phải train cross-encoder trên các hard negative trả về từ bi-encoder.\n3. Sử dụng mô hình **bi_encoder_xlmRoBERTA** trích xuất đặc trưng trên toàn bộ corpus và lưu vào FAISS vector database để lưu trữ và fast silimarity search. Vector embedding được chứa trong **bi_encoder_corpus_embedding**\n4. Định nghĩa một mô hình cross-encoder cơ bản dùng thư viện sentence_transformer với base model là XML-RoBERTA và huấn luyện nó trên bộ dữ liệu **cross-encoder-dataset**, khi huấn luyện nhận thấy kết quả không thay đổi nhiều nên đã dừng train sau khoảng 200000 sample, kết quả thu được mô hình **cross_encoder_ckp** dùng để reranking.\n5. Định nghĩa một pipeline để load các mô hình đã train được để sử dụng và đánh giá hiệu quả mô hình.","metadata":{}},{"cell_type":"markdown","source":"### Một số ý tưởng cách cải thiện mô hình\n1. Cải thiện Bi-encoder:\n   * Train lại bi-encoder dùng base model được train riêng biệt cho tiếng việt như PhoBERT https://huggingface.co/vinai/phobert-base. dùng các thư viện như underthesea để word segmentation dataset trước khi train\n   * Thay vì code chay bi-encoder ta có thể dùng một bi-encoder được pretrain sẵn cho tiếng Việt như https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder, nếu dùng bi-encoder được pretrain cần phải viết lại một hàm mới để load và train mô hình và word segmentation nếu cần thiết\n   * Train thêm nhiều epoch và mine hard negative, mô hình hiện tại dùng XML-RoBERTA và chỉ train trên 1 epoch chưa mine hard negative vì bị tràn ram\n2. Cải thiện Cross-encoder:\n   * Tạo lại dataset mới để train cho cross-encoder từ mô hình bi-encoder đã được cải thiện\n   * Với dataset **cross-encoder-dataset** hiện tại chỉ có 2 class là 1 cho ground truth và 0 cho cả hard và random negative. Ta có thể tạo thêm 1 class nữa với label là 2 cho random negative để giảm sự mất cân bằng dữ liệu.\n   * Tìm thêm các cách tốt hơn để tạo dataset để train cross-encoder.\n   * Thay vì dùng base model là XLM-RoBERTA nên chuyển sang dùng thử PhoBERT.","metadata":{}},{"cell_type":"markdown","source":"### Các thay đổi đã được thực hiện\n1. Sử dụng underthesea để thực hiện word segmentation cho train.csv và corpus.csv, thu được tập dữ liệu **soict-dataset-2024-segmented**\n2. Dùng https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder để làm base model và train một mô hình bi-encoder mới trên tập dữ **soict-dataset-2024-segmented**, mô hình bi-encoder mới được train trên 2 epochs có mining hard negative và layers freezing, (xem method train của BiEncoderTrainer)\n3. Dùng bi-encoder mới để tạo bộ dữ liệu mới là **cross-encoder-segmented** để train một mô hình cross-encoder mới\n4. Train mô hình cross-encoder mới với base model tương tự như bi-encoder","metadata":{}},{"cell_type":"markdown","source":"## STAGE 1 - Bi-encoder (First Retrieval Stage)","metadata":{}},{"cell_type":"code","source":"!pip install --quiet faiss-gpu sentence-transformers underthesea","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:18.442012Z","iopub.execute_input":"2025-01-09T15:14:18.442264Z","iopub.status.idle":"2025-01-09T15:14:27.974628Z","shell.execute_reply.started":"2025-01-09T15:14:18.442242Z","shell.execute_reply":"2025-01-09T15:14:27.973624Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m657.8/657.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import XLMRobertaModel, XLMRobertaTokenizer, AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Union\nimport faiss\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nclass BiEncoderConfig:\n    def __init__(\n        self,\n        max_length: int = 256,\n        batch_size: int = 16,\n        learning_rate: float = 1e-5,\n        num_epochs: int = 2,\n        temperature: float = 0.05,\n        embedding_dim: int = 768,\n\n    ):\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.temperature = temperature\n        self.embedding_dim = embedding_dim\n\nclass LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }\nclass BiEncoder(nn.Module):\n    def __init__(self, config: BiEncoderConfig):\n        super().__init__()\n        self.config = config\n        \n        # Load the pre-trained Vietnamese bi-encoder for both encoders\n        self.question_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.document_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n\n        # Use the model's tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.max_length = config.max_length\n\n    def get_device(self):\n        # Helper method to get the current device\n        if isinstance(self.question_encoder, nn.DataParallel):\n            return self.question_encoder.module.device\n        return next(self.question_encoder.parameters()).device\n\n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(\n            -1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def encode_question(self, questions: List[str], batch_size: int = 32) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        for i in range(0, len(questions), batch_size):\n            batch_texts = questions[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.question_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n    def encode_document(self, documents: List[str], batch_size: int = 32, disable_progress_bar: bool = False) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        # Calculate total number of batches for progress bar\n        num_batches = (len(documents) + batch_size - 1) // batch_size\n\n        for i in tqdm(range(0, len(documents), batch_size), total=num_batches, desc=\"Encoding documents\", disable=disable_progress_bar):\n            batch_texts = documents[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.document_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n\nclass LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:27.975636Z","iopub.execute_input":"2025-01-09T15:14:27.975863Z","iopub.status.idle":"2025-01-09T15:14:34.417171Z","shell.execute_reply.started":"2025-01-09T15:14:27.975843Z","shell.execute_reply":"2025-01-09T15:14:34.416497Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%writefile cross_encoder.py\nimport torch\nfrom torch import nn\nfrom transformers import XLMRobertaModel, XLMRobertaTokenizer, AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Union\nimport faiss\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\nclass BiEncoderConfig:\n    def __init__(\n        self,\n        max_length: int = 256,\n        batch_size: int = 16,\n        learning_rate: float = 1e-5,\n        num_epochs: int = 2,\n        temperature: float = 0.05,\n        embedding_dim: int = 768,\n\n    ):\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.temperature = temperature\n        self.embedding_dim = embedding_dim\n\nclass LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }\n    \nclass BiEncoder(nn.Module):\n    def __init__(self, config: BiEncoderConfig):\n        super().__init__()\n        self.config = config\n        \n        # Load the pre-trained Vietnamese bi-encoder for both encoders\n        self.question_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.document_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n\n        # Use the model's tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.max_length = config.max_length\n\n    def get_device(self):\n        # Helper method to get the current device\n        if isinstance(self.question_encoder, nn.DataParallel):\n            return self.question_encoder.module.device\n        return next(self.question_encoder.parameters()).device\n\n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(\n            -1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def encode_question(self, questions: List[str], batch_size: int = 32) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        for i in range(0, len(questions), batch_size):\n            batch_texts = questions[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.question_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n    def encode_document(self, documents: List[str], batch_size: int = 32, disable_progress_bar: bool = False) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        # Calculate total number of batches for progress bar\n        num_batches = (len(documents) + batch_size - 1) // batch_size\n\n        for i in tqdm(range(0, len(documents), batch_size), total=num_batches, desc=\"Encoding documents\", disable=disable_progress_bar):\n            batch_texts = documents[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.document_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n\nclass LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }\n    \nclass BiEncoderTrainer:\n\n    def get_config(self) -> BiEncoderConfig:\n        \"\"\"Get the trainer's configuration\"\"\"\n        return self.config\n        \n    def get_model(self) -> BiEncoder:\n        \"\"\"Get the trainer's model\"\"\"\n        return self.model\n        \n    def __init__(self, config: BiEncoderConfig):\n        self.config = config\n        self.model = BiEncoder(self.config)\n        \n        # Initialize with smaller learning rate for fine-tuning\n        self.config.learning_rate = 1e-5  # Reduced from 2e-5\n        \n        # Setup multi-GPU\n        if torch.cuda.device_count() > 1:\n            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n            self.model.question_encoder = nn.DataParallel(self.model.question_encoder)\n            self.model.document_encoder = nn.DataParallel(self.model.document_encoder)\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        \n        # Implement gradual unfreezing\n        self.unfreeze_layers = 0  # Start with all layers frozen\n        self._freeze_layers()\n        \n        # metrics tracking\n        self.best_mrr = 0.0\n        self.best_recall = 0.0\n        \n        # Add new attributes for step-based unfreezing\n        self.total_steps = 0\n        self.unfreeze_schedule = None  # Will be set in train()\n\n    def _freeze_layers(self):\n        \"\"\"Freeze/unfreeze layers gradually during training\"\"\"\n        # First freeze all layers\n        for param in self.model.question_encoder.parameters():\n            param.requires_grad = False\n        for param in self.model.document_encoder.parameters():\n            param.requires_grad = False\n            \n        def unfreeze_model_layers(model, num_layers):\n            # Always unfreeze the pooler and final layer\n            if isinstance(model, nn.DataParallel):\n                model = model.module\n            \n            # Unfreeze pooler\n            for param in model.pooler.parameters():\n                param.requires_grad = True\n            \n            # Always keep the final layer unfrozen\n            if num_layers == 0:\n                for param in model.encoder.layer[-1].parameters():\n                    param.requires_grad = True\n                return\n                \n            # Unfreeze specified number of layers from the top\n            for layer in list(model.encoder.layer)[-num_layers:]:\n                for param in layer.parameters():\n                    param.requires_grad = True\n                \n        # Apply unfreezing to both encoders\n        unfreeze_model_layers(self.model.question_encoder, self.unfreeze_layers)\n        unfreeze_model_layers(self.model.document_encoder, self.unfreeze_layers)\n\n    def set_scores(self, scores: Tuple):\n        self.best_mrr = scores[0]\n        self.best_recall = scores[1]\n        \n    def prepare_batch(self, batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n        # tokenize questions\n        questions_tokenized = self.model.tokenizer(\n            batch['question'],\n            padding=True,\n            truncation=True,\n            max_length=self.config.max_length,\n            return_tensors='pt'\n        ).to(self.device)\n        \n        #Tokenize contexts\n        contexts_tokenized = self.model.tokenizer(\n            batch['context'],\n            padding=True,\n            truncation=True,\n            max_length=self.config.max_length,\n            return_tensors='pt'\n        ).to(self.device)\n        \n        return {\n            'question_data': questions_tokenized,\n            'context_data': contexts_tokenized\n        }\n    \n    def compute_loss(self, q_embeddings: torch.Tensor, d_embeddings: torch.Tensor,\n                    hard_negative_embeddings: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Compute loss with both in-batch and hard negatives\"\"\"\n        #regular in-batch negative loss\n        similarity = torch.matmul(q_embeddings, d_embeddings.t())\n        \n        if hard_negative_embeddings is not None:\n            # hard negative similarities\n            hard_similarity = torch.matmul(q_embeddings, hard_negative_embeddings.t())\n            similarity = torch.cat([similarity, hard_similarity], dim=1)\n        \n        # scale by temperature\n        similarity = similarity / self.config.temperature\n        \n        # Create labels for diagonal (positive pairs)\n        labels = torch.arange(q_embeddings.size(0)).to(self.device)\n        \n        # Compute loss\n        loss = nn.CrossEntropyLoss()(similarity, labels)\n        \n        return loss\n    \n    def evaluate(self, val_dataset: LegalDataset) -> Dict[str, float]:\n        \"\"\"\n        Evaluate the model on validation dataset\n        Returns MRR@k and Recall@k metrics\n        \"\"\"\n        self.model.eval()\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False\n        )\n        \n        all_q_embeddings = []\n        all_d_embeddings = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n                processed_batch = self.prepare_batch(batch)\n                \n                # Get embeddings\n                q_embeddings = self.model.mean_pooling(\n                    self.model.question_encoder(**processed_batch['question_data']),\n                    processed_batch['question_data']['attention_mask']\n                )\n                d_embeddings = self.model.mean_pooling(\n                    self.model.document_encoder(**processed_batch['context_data']),\n                    processed_batch['context_data']['attention_mask']\n                )\n                \n                # Normalize\n                q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n                d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n                \n                all_q_embeddings.append(q_embeddings)\n                all_d_embeddings.append(d_embeddings)\n        \n        # Concatenate all embeddings\n        all_q_embeddings = torch.cat(all_q_embeddings, dim=0)\n        all_d_embeddings = torch.cat(all_d_embeddings, dim=0)\n        \n        # Compute similarity matrix\n        similarity = torch.matmul(all_q_embeddings, all_d_embeddings.t())\n        \n        # Calculate metrics\n        k_values = [1, 5, 10, 50,100, 200, 500, 1000]\n        metrics = {}\n        \n        for k in k_values:\n            # Get top-k indices\n            _, indices = similarity.topk(k, dim=1)\n            \n            # Calculate Recall@k\n            correct = torch.arange(similarity.size(0)).unsqueeze(1).expand_as(indices).to(self.device)\n            recall_at_k = (indices == correct).float().sum(dim=1).mean().item()\n            metrics[f'recall@{k}'] = recall_at_k\n            \n            # Calculate MRR@k\n            rank = (indices == correct).nonzero()[:, 1] + 1\n            mrr = (1.0 / rank).mean().item()\n            metrics[f'mrr@{k}'] = mrr\n            \n        return metrics\n\n    def train(self, train_dataset: LegalDataset, val_dataset: LegalDataset = None):\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True\n        )\n        \n        # Calculate total steps and set unfreeze schedule\n        total_steps = len(train_dataloader) * self.config.num_epochs\n        steps_per_epoch = len(train_dataloader)\n        \n        # Adjust unfreeze schedule to be more frequent within the epoch\n        self.unfreeze_schedule = {\n            steps_per_epoch // 4: 2,     # Unfreeze top 2 layers after 25% steps\n            steps_per_epoch // 2: 4,     # Unfreeze top 4 layers after 50% steps\n            3 * steps_per_epoch // 4: 6,  # Unfreeze top 6 layers after 75% steps\n            9 * steps_per_epoch // 10: 8  # Unfreeze top 8 layers after 90% steps\n        }\n        \n        # Use different optimizers for frozen/unfrozen parameters\n        def get_optimizer():\n            params = []\n            for model in [self.model.question_encoder, self.model.document_encoder]:\n                params.extend([p for p in model.parameters() if p.requires_grad])\n            return torch.optim.AdamW(\n                params,\n                lr=self.config.learning_rate,\n                weight_decay=0.01\n            )\n        \n        optimizer = get_optimizer()\n        \n        # Add learning rate scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, \n            T_max=total_steps\n        )\n        \n        # Adjust mining frequency to occur multiple times within epoch\n        mine_every_n_steps = steps_per_epoch // 2  # Mine 4 times per epoch\n        \n        for epoch in range(self.config.num_epochs):\n            self.model.train()\n            total_loss = 0\n            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{self.config.num_epochs}')\n            \n            hard_negatives = None\n            current_epoch_step = 0\n            \n            for batch_idx, batch in enumerate(progress_bar):\n                current_epoch_step = batch_idx\n                \n                # Check if it's time to mine hard negatives\n                if current_epoch_step % mine_every_n_steps == 0 and current_epoch_step != 0:\n                    print(f\"\\nMining hard negatives at step {current_epoch_step}...\")\n                    questions = train_dataset.questions\n                    contexts = train_dataset.contexts\n                    hard_negatives = self.mine_hard_negatives(questions, contexts)\n                \n                # Check unfreeze schedule based on current epoch step\n                if current_epoch_step in self.unfreeze_schedule:\n                    print(f\"\\nUnfreezing layers at step {current_epoch_step}...\")\n                    self.unfreeze_layers = self.unfreeze_schedule[current_epoch_step]\n                    self._freeze_layers()\n                    optimizer = get_optimizer()  # Reinitialize optimizer with new trainable params\n                \n                optimizer.zero_grad()\n                \n                #Prepare batch data\n                processed_batch = self.prepare_batch(batch)\n                \n                #Get embeddings from separate encoders\n                q_embeddings = self.model.mean_pooling(\n                    self.model.question_encoder(**processed_batch['question_data']),\n                    processed_batch['question_data']['attention_mask']\n                )\n                d_embeddings = self.model.mean_pooling(\n                    self.model.document_encoder(**processed_batch['context_data']),\n                    processed_batch['context_data']['attention_mask']\n                )\n                \n                #Process hard negatives if available\n                hard_negative_embeddings = None\n                if hard_negatives is not None:\n                    hard_negative_batch = self.model.tokenizer(\n                        hard_negatives[batch_idx:batch_idx + len(batch)],\n                        padding=True,\n                        truncation=True,\n                        max_length=self.config.max_length,\n                        return_tensors='pt'\n                    ).to(self.device)\n                    \n                    hard_negative_embeddings = self.model.mean_pooling(\n                        self.model.document_encoder(**hard_negative_batch),\n                        hard_negative_batch['attention_mask']\n                    )\n                    hard_negative_embeddings = nn.functional.normalize(hard_negative_embeddings, p=2, dim=1)\n                \n                #Normalize embeddings\n                q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n                d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n                \n                #Compute loss with hard negatives\n                loss = self.compute_loss(q_embeddings, d_embeddings, hard_negative_embeddings)\n                \n                #Backward pass\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n                progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n            \n            avg_loss = total_loss / len(train_dataloader)\n            print(f'Epoch {epoch + 1}/{self.config.num_epochs}, Average Loss: {avg_loss:.4f}')\n            \n            # Validation step\n            if val_dataset is not None:\n                metrics = self.evaluate(val_dataset)\n                print(\"Validation metrics:\")\n                for metric_name, value in metrics.items():\n                    print(f\"{metric_name}: {value:.4f}\")\n                \n                # Save best model\n                if metrics['mrr@10'] > self.best_mrr:\n                    self.best_mrr = metrics['mrr@10']\n                    self.save_model('best_model.pt')\n    \n    def save_model(self, path: str):\n        # Save both encoders and config\n        torch.save({\n            'question_encoder': self.model.question_encoder.state_dict(),\n            'document_encoder': self.model.document_encoder.state_dict(),\n            'config': self.config\n        }, path)\n    \n    def load_model(self, path: str):\n        checkpoint = torch.load(path)\n        print\n        question_state_dict = checkpoint['question_encoder']\n        document_state_dict = checkpoint['document_encoder']\n        \n        # Remove 'module.' prefix if it exists and model is not using DataParallel\n        if not isinstance(self.model.question_encoder, nn.DataParallel):\n            print(\"remove module.\")\n            question_state_dict = {k.replace('module.', ''): v for k, v in question_state_dict}\n            document_state_dict = {k.replace('module.', ''): v for k, v in document_state_dict}\n        # Add 'module.' prefix if model is using DataParallel but saved model wasn't\n        elif not any(k.startswith('module.') for k in question_state_dict):\n            print(\"add module.\")\n            question_state_dict = {'module.' + k: v for k, v in question_state_dict}\n            document_state_dict = {'module.' + k: v for k, v in document_state_dict}\n        \n        # Load the state dictionaries\n        try:\n            self.model.question_encoder.load_state_dict(question_state_dict)\n            self.model.document_encoder.load_state_dict(document_state_dict)\n        except RuntimeError as e:\n            print(f\"Error loading state dict: {e}\")\n            print(\"Attempting alternative loading method...\")\n            \n            # If the first attempt fails, try the opposite approach\n            if isinstance(self.model.question_encoder, nn.DataParallel):\n                question_state_dict = {k.replace('module.', ''): v for k, v in question_state_dict.items()}\n                document_state_dict = {k.replace('module.', ''): v for k, v in document_state_dict.items()}\n            else:\n                question_state_dict = {'module.' + k: v for k, v in question_state_dict.items()}\n                document_state_dict = {'module.' + k: v for k, v in document_state_dict.items()}\n            \n            self.model.question_encoder.load_state_dict(question_state_dict)\n            self.model.document_encoder.load_state_dict(document_state_dict)\n\n    def mine_hard_negatives(self, questions: List[str], documents: List[str], \n                       batch_size: int = 256) -> List[str]:  # Reduced batch size\n        \"\"\"Mine hard negatives using embeddings similarity\"\"\"\n        self.model.eval()\n        device = self.device\n    \n        # Significantly reduced chunk sizes to avoid OOM\n        chunk_size = 2000  # Reduced from 5000\n        similarity_chunk_size = 200  # Reduced from 500\n    \n        with torch.no_grad():\n            # Process questions in smaller chunks\n            all_q_embeddings = []\n            for i in tqdm(range(0, len(questions), chunk_size), desc=\"Encoding questions\"):\n                q_chunk = questions[i:i + chunk_size]\n                q_emb = self.model.encode_question(q_chunk, batch_size)\n                all_q_embeddings.append(q_emb.cpu())  # Move to CPU after processing\n                torch.cuda.empty_cache()\n            q_embeddings = torch.cat(all_q_embeddings, dim=0)\n        \n            # Process documents in smaller chunks\n            all_d_embeddings = []\n            for i in tqdm(range(0, len(documents), chunk_size), desc=\"Encoding documents\"):\n                d_chunk = documents[i:i + chunk_size]\n                d_emb = self.model.encode_document(d_chunk, batch_size)\n                all_d_embeddings.append(d_emb.cpu())  # Move to CPU after processing\n                torch.cuda.empty_cache()\n            d_embeddings = torch.cat(all_d_embeddings, dim=0)\n            \n            # Normalize embeddings (on CPU to save GPU memory)\n            q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n            d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n            \n            # Compute similarity in smaller chunks\n            hard_negative_indices = []\n            k = 2  # Reduced number of hard negatives per question\n            \n            for i in tqdm(range(0, len(q_embeddings), similarity_chunk_size), desc=\"Mining negatives\"):\n                # Move only the current chunks to GPU\n                q_chunk = q_embeddings[i:i + similarity_chunk_size].to(device)\n                d_chunk = d_embeddings.to(device)\n                \n                similarity = torch.matmul(q_chunk, d_chunk.t())\n                \n                # Get top-k most similar but incorrect documents\n                values, indices = similarity.topk(k + 1, dim=1)\n                \n                # Filter out positive pairs\n                mask = torch.arange(i, min(i + similarity_chunk_size, len(q_embeddings))).unsqueeze(1).expand_as(indices).to(device)\n                chunk_negative_indices = indices[indices != mask].view(-1)\n                hard_negative_indices.extend(chunk_negative_indices.cpu().numpy())\n                \n                # Free memory\n                del similarity, values, indices, q_chunk, d_chunk\n                torch.cuda.empty_cache()\n        \n        return [documents[idx] for idx in hard_negative_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:34.418623Z","iopub.execute_input":"2025-01-09T15:14:34.419039Z","iopub.status.idle":"2025-01-09T15:14:34.427961Z","shell.execute_reply.started":"2025-01-09T15:14:34.419016Z","shell.execute_reply":"2025-01-09T15:14:34.427093Z"}},"outputs":[{"name":"stdout","text":"Writing cross_encoder.py\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%writefile dual_encoder.py\nfrom sentence_transformers import CrossEncoder\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sentence_transformers.readers import InputExample\nfrom sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\nfrom sentence_transformers import LoggingHandler\nimport logging\nimport pandas as pd\nimport torch\n\n\nclass LoggingCallback:\n    def __init__(self):\n        self.current_step = 0\n\n    def __call__(self, score: float, epoch: int, steps: int):\n        self.current_step += 1\n        print(f'Step: {self.current_step}, Epoch: {epoch}, Loss: {score:.4f}')\n\n\nclass CrossEncoderWrapper:\n    def __init__(\n        self,\n        model_name: str = \"bkai-foundation-models/vietnamese-bi-encoder\",\n        max_length: int = 256,\n        batch_size: int = 16,\n    ):\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.max_length = max_length\n        \n        # Explicitly set device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize model with explicit device\n        self.model = CrossEncoder(\n            model_name,\n            max_length=max_length,\n            device=self.device\n        )\n\n    def train(self, train_dataset: Dataset, val_dataset: Dataset = None,\n              num_epochs: int = 1, warmup_ratio: float = 0.02):\n        \"\"\"Train the cross-encoder model\"\"\"\n\n       # Setup logging\n        logging.basicConfig(format='%(asctime)s - %(message)s',\n                            datefmt='%Y-%m-%d %H:%M:%S',\n                            level=logging.INFO,\n                            handlers=[LoggingHandler()])\n\n        # Prepare training examples\n        train_samples = [\n            InputExample(texts=[sample['question'], sample['document']],\n                         label=sample['label'])\n            for sample in train_dataset\n        ]\n\n        # Create data loader\n        train_dataloader = DataLoader(\n            train_samples,\n            shuffle=True,\n            batch_size=self.batch_size\n        )\n\n        # Create evaluator for validation\n        if val_dataset is not None:\n            # Prepare validation data in the required format\n            val_samples = [\n                InputExample(texts=[sample['question'], sample['document']],\n                             label=sample['label'])\n                for sample in val_dataset\n            ]\n\n            evaluator = CEBinaryAccuracyEvaluator.from_input_examples(\n                val_samples, name=\"evaluation\")\n        else:\n            evaluator = None\n        # Create callback\n        callback = LoggingCallback()\n\n        # Train the model\n        self.model.fit(\n            train_dataloader=train_dataloader,\n            evaluator=evaluator,\n            epochs=num_epochs,\n            evaluation_steps=2000,  # Evaluate every 2000 steps\n            warmup_steps=int(len(train_dataset) * warmup_ratio),\n            show_progress_bar=True,\n            output_path='checkpoints',  # Save checkpoints\n            # save_best_model=True,\n            callback=callback  # Add the callback\n        )\n\n    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n        \"\"\"Evaluate the model on a dataset\"\"\"\n        pairs = [\n            [sample['question'], sample['document']]\n            for sample in dataset\n        ]\n        labels = [sample['label'] for sample in dataset]\n\n        # Get predictions\n        scores = self.model.predict(pairs)\n        predictions = (scores > 0.5).astype(int)\n\n        # Calculate metrics\n        metrics = {\n            'accuracy': accuracy_score(labels, predictions),\n            'precision': precision_score(labels, predictions),\n            'recall': recall_score(labels, predictions),\n            'f1': f1_score(labels, predictions),\n            'auc_roc': roc_auc_score(labels, scores)\n        }\n\n        return metrics\n\n    def predict(self, questions: List[str], documents: List[str], show_progress_bar=True) -> np.ndarray:\n        \"\"\"Get relevance scores for question-document pairs\"\"\"\n        try:\n            # Ensure model is in eval mode\n            self.model.model.eval()\n            \n            # Create pairs\n            pairs = [[q, d] for q, d in zip(questions, documents)]\n\n            # Make prediction with smaller batch size\n            return self.model.predict(\n                pairs,\n                batch_size=16,  # Smaller batch size\n                show_progress_bar=show_progress_bar\n            )\n            \n        except Exception as e:\n            print(f\"Error during prediction: {str(e)}\")\n            raise\n\n    def save_model(self, path: str):\n        self.model.save(path)\n\n    def load_model(self, path: str):\n        self.model = CrossEncoder(\n            path,\n            max_length=self.max_length,\n            device=self.device\n        )\n\n\nclass CrossEncoderDataset(Dataset):\n    def __init__(self, questions: List[str], documents: List[str], labels: List[int]):\n        self.questions = questions\n        self.documents = documents\n        self.labels = labels\n        assert len(questions) == len(documents) == len(\n            labels), \"All inputs must have the same length\"\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        return {\n            'question': self.questions[idx],\n            'document': self.documents[idx],\n            'label': self.labels[idx]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:34.429337Z","iopub.execute_input":"2025-01-09T15:14:34.429642Z","iopub.status.idle":"2025-01-09T15:14:34.447109Z","shell.execute_reply.started":"2025-01-09T15:14:34.429619Z","shell.execute_reply":"2025-01-09T15:14:34.446471Z"}},"outputs":[{"name":"stdout","text":"Writing dual_encoder.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"%%writefile pipeline.py\nfrom underthesea import word_tokenize\nfrom cross_encoder import *\nfrom dual_encoder import *\n\n\nclass RetrievalPipeline:\n    def __init__(self,\n                 bi_encoder_path: str,\n                 cross_encoder_path: str,\n                 faiss_index_path: str,\n                 # embeddings_path: str,\n                 corpus_df_path: str,\n                 top_k: int = 50,\n                 rerank_k: int = 10):\n        \"\"\"\n        Initialize retrieval pipeline\n        Args:\n            bi_encoder_path: Path to bi encoder checkpoint\n            cross_encoder_path: Path to cross encoder checkpoint\n            faiss_index_path: Path to saved FAISS index\n            embeddings_path: Path to saved document embeddings\n            corpus_df_path: Path to corpus CSV file\n            top_k: Number of candidates to retrieve from FAISS\n            rerank_k: Number of final results after reranking\n        \"\"\"\n        # Load models\n        self.bi_encoder = BiEncoder(BiEncoderConfig())\n        self.cross_encoder = CrossEncoderWrapper()\n\n        # Load model\n        self.load_models(bi_encoder_path, cross_encoder_path)\n\n        # Load FAISS index and embeddings\n        self.index = faiss.read_index(faiss_index_path)\n        # self.document_embeddings = np.load(embeddings_path)\n\n        # Load corpus for retrieving text\n        self.corpus_df = pd.read_csv(corpus_df_path)\n\n        # Configuration\n        self.top_k = top_k\n        self.rerank_k = rerank_k\n\n        # Move models to GPU if available\n        self.device = torch.device(\n            'cuda' if torch.cuda.is_available() else 'cpu')\n        self.bi_encoder.to(self.device)\n        # self.cross_encoder.model.to(torch.device(\"cuda\"))\n\n    def load_models(self, bi_encoder_path: str, cross_encoder_path: str):\n        bi_encoder_checkpoint = torch.load(bi_encoder_path)\n        question_state_dict = bi_encoder_checkpoint['question_encoder']\n        document_state_dict = bi_encoder_checkpoint['document_encoder']\n\n        question_state_dict = {\n            k.replace('module.', ''): v for k, v in question_state_dict.items()}\n        document_state_dict = {\n            k.replace('module.', ''): v for k, v in document_state_dict.items()}\n\n        self.bi_encoder.document_encoder.load_state_dict(\n            document_state_dict)\n        self.bi_encoder.question_encoder.load_state_dict(\n            question_state_dict)\n        # self.bi_encoder.config = bi_encoder_checkpoint['config']\n\n        self.cross_encoder.load_model(cross_encoder_path)\n\n    def retrieve(self, query: str, rerank: bool = True) -> List[Dict[str, Union[str, float, int]]]:\n        \"\"\"\n        Retrieve and rerank documents for a query\n        Args:\n            query: Question string\n        Returns:\n            List of dicts containing retrieved documents with scores and metadata\n        \"\"\"\n        # Clear cache\n        torch.cuda.empty_cache()\n\n        # Stage 1: Bi-encoder retrieval\n        query_embedding = self.bi_encoder.encode_question([query])\n\n        query_embedding = query_embedding.cpu().numpy()\n        faiss.normalize_L2(query_embedding)\n\n        # Search FAISS index\n        scores, doc_indices = self.index.search(query_embedding, self.top_k)\n\n        # Get candidate documents\n        candidates = []\n        for score, doc_idx in zip(scores[0], doc_indices[0]):\n            candidates.append({\n                'text': self.corpus_df.iloc[doc_idx]['text'],\n                'cid': self.corpus_df.iloc[doc_idx]['cid'],\n                'bi_encoder_score': float(score)\n            })\n        if rerank:\n            # Stage 2: Cross-encoder reranking\n            candidate_texts = [c['text'] for c in candidates]\n            # self.bi_encoder.to(torch.device(\"cuda\"))\n            batch_size = 16  # Adjust this based on your GPU memory capacity\n            cross_encoder_scores = []\n\n            for i in range(0, len(candidate_texts), batch_size):\n                batch_texts = candidate_texts[i:i + batch_size]\n                batch_scores = self.cross_encoder.predict(\n                    [query] * len(batch_texts),\n                    batch_texts,\n                    show_progress_bar=False\n                )\n                cross_encoder_scores.extend(batch_scores)\n            # self.cross_encoder.to(torch.device(\"cpu\"))\n            # Add cross-encoder scores\n            for idx, score in enumerate(cross_encoder_scores):\n                candidates[idx]['cross_encoder_score'] = float(score)\n\n            candidates.sort(\n                key=lambda x: x['cross_encoder_score'], reverse=True)\n            results = candidates[:self.rerank_k]\n\n            return results\n\n        else:\n            candidates.sort(key=lambda x: x['bi_encoder_score'], reverse=True)\n            results = candidates[:self.rerank_k]\n\n            return results\n\n    def batch_retrieve(self,\n                       queries: List[str],\n                       batch_size: int = 32) -> List[List[Dict[str, Union[str, float, int]]]]:\n        \"\"\"\n        Batch retrieval for multiple queries\n        \"\"\"\n        all_results = []\n        for i in range(0, len(queries), batch_size):\n            batch = queries[i:i + batch_size]\n            batch_results = [self.retrieve(q) for q in batch]\n            all_results.extend(batch_results)\n        return all_results\n\n\ndef evaluate_retrieval(pipeline, test_df, top_k=10, re_ranking=True):\n    \"\"\"\n    Evaluate the retrieval pipeline on a test set using MAP, MRR, NDCG, and Recall@k.\n\n    Args:\n        pipeline: The retrieval pipeline instance.\n        test_df: DataFrame containing 'question' and 'cid' columns.\n        top_k: Number of top results to consider for evaluation.\n\n    Returns:\n        A dictionary with MAP, MRR, NDCG, and Recall@k scores.\n    \"\"\"\n    def parse_cid_string(cid_str):\n        \"\"\"Parse a space-separated string of CIDs into a set of integers.\"\"\"\n        return set(map(int, cid_str.strip('[]').split()))\n\n    true_cids = test_df['cid'].apply(parse_cid_string)\n    questions = test_df['question'].tolist()\n\n    average_precisions = []\n    reciprocal_ranks = []\n    ndcg_scores = []\n    recall_at_k = []\n\n    for question, true_cid_set in tqdm(zip(questions, true_cids), total=len(questions), desc=\"Evaluating\"):\n        # Retrieve documents\n        results = pipeline.retrieve(question, re_ranking)\n\n        # Get retrieved cids\n        retrieved_cids = [result['cid'] for result in results[:top_k]]\n\n        # Calculate Average Precision\n        num_relevant = 0\n        precision_sum = 0.0\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                num_relevant += 1\n                precision_sum += num_relevant / (i + 1)\n        average_precision = precision_sum / \\\n            len(true_cid_set) if true_cid_set else 0\n        average_precisions.append(average_precision)\n\n        # Calculate Reciprocal Rank\n        reciprocal_rank = 0.0\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                reciprocal_rank = 1.0 / (i + 1)\n                break\n        reciprocal_ranks.append(reciprocal_rank)\n\n        # Calculate NDCG\n        dcg = 0.0\n        idcg = sum(1.0 / (i + 1) for i in range(min(len(true_cid_set), top_k)))\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                dcg += 1.0 / (i + 1)\n        ndcg = dcg / idcg if idcg > 0 else 0\n        ndcg_scores.append(ndcg)\n\n        # Calculate Recall@k\n        relevant_retrieved = len(set(retrieved_cids) & true_cid_set)\n        recall = relevant_retrieved / len(true_cid_set) if true_cid_set else 0\n        recall_at_k.append(recall)\n\n    # Calculate average metrics\n    map_score = sum(average_precisions) / len(average_precisions)\n    mrr_score = sum(reciprocal_ranks) / len(reciprocal_ranks)\n    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n    avg_recall_at_k = sum(recall_at_k) / len(recall_at_k)\n\n    return {\n        'MAP': map_score,\n        'MRR': mrr_score,\n        'NDCG': avg_ndcg,\n        'Recall@k': avg_recall_at_k\n    }\n\n\nbi_encoder_path = \"/kaggle/working/bi_encoder.pt\"\ncross_encoder_path = \"/kaggle/input/cross_encoder_new/pytorch/default/1\"\nfaiss_index_path = \"/kaggle/input/bi_encoder_embedding/pytorch/default/1/document_index.faiss\"\ncorpus_df_path = \"/kaggle/input/soict-dataset-2024-segmented/corpus_segmented.csv\"\n\n# Initialize pipeline\npipeline = RetrievalPipeline(\n    bi_encoder_path=bi_encoder_path,\n    cross_encoder_path=cross_encoder_path,\n    faiss_index_path=faiss_index_path,\n    # embeddings_path=\"path/to/embeddings.npy\",\n    corpus_df_path=corpus_df_path,\n    # rerank_k=50\n)\n\n\ndef segment(text: str) -> str:\n    return word_tokenize(text, format=\"text\")\n\n\ndef format_retrieval_results(results: List[Dict[str, Union[str, float, int]]]) -> str:\n    \"\"\"\n    Reformat and concatenate retrieved documents into a single string.\n\n    Args:\n        results: List of dictionaries containing retrieved documents with their metadata\n\n    Returns:\n        str: Formatted and concatenated document string\n    \"\"\"\n    # Remove quotes and underscores, then join documents with newlines\n    formatted_docs = []\n\n    for doc in results:\n        # Remove quotes and underscores from text\n        text = doc['text'].strip('\"\\'')  # Remove quotes\n        text = text.replace('_', ' ')    # Replace underscores with spaces\n\n        # Add document to list with its score\n        formatted_docs.append(f\"Document (relevance score: {doc['cross_encoder_score']:.2f}):\\n{text}\")\n\n    # Join all documents with double newlines for better readability\n    return \"\\n\\n\".join(formatted_docs)\n\n\ndef retrieval_legal_documents(query: str) -> str:\n    \"\"\"\n    Sử dụng công cụ này để truy vấn 10 tài liệu luật liên quan nhất đến câu query\n    Ví dụ cách sử dụng:\n    query: Quy định về kinh doanh như thế nào?\n    \"\"\"\n    segment_query = segment(query)\n    results = pipeline.retrieve(segment_query)\n    return format_retrieval_results(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:34.448082Z","iopub.execute_input":"2025-01-09T15:14:34.448403Z","iopub.status.idle":"2025-01-09T15:14:34.465609Z","shell.execute_reply.started":"2025-01-09T15:14:34.448352Z","shell.execute_reply":"2025-01-09T15:14:34.464842Z"}},"outputs":[{"name":"stdout","text":"Writing pipeline.py\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Create a script to clean the checkpoint\nimport torch\n\n# Load the checkpoint\ncheckpoint = torch.load(\"/kaggle/input/bi_encoder/pytorch/default/1/bi_encoder.pt\")\n\n# Create new checkpoint with only the model states\nclean_checkpoint = {\n    'question_encoder': checkpoint['question_encoder'],\n    'document_encoder': checkpoint['document_encoder']\n}\n\n# Save the clean checkpoint\ntorch.save(clean_checkpoint, 'bi_encoder.pt')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:34.466468Z","iopub.execute_input":"2025-01-09T15:14:34.466751Z","iopub.status.idle":"2025-01-09T15:14:45.053724Z","shell.execute_reply.started":"2025-01-09T15:14:34.466721Z","shell.execute_reply":"2025-01-09T15:14:45.053007Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-f250fcdf8178>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(\"/kaggle/input/bi_encoder/pytorch/default/1/bi_encoder.pt\")\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"## STAGE 2 - Cross-encoder (Reranking Stage):","metadata":{}},{"cell_type":"markdown","source":"## Interface","metadata":{}},{"cell_type":"code","source":"!pip install pydantic==2.10","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:45.054581Z","iopub.execute_input":"2025-01-09T15:14:45.054882Z","iopub.status.idle":"2025-01-09T15:14:51.164140Z","shell.execute_reply.started":"2025-01-09T15:14:45.054852Z","shell.execute_reply":"2025-01-09T15:14:51.163239Z"}},"outputs":[{"name":"stdout","text":"Collecting pydantic==2.10\n  Downloading pydantic-2.10.0-py3-none-any.whl.metadata (167 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.8/167.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.10) (0.7.0)\nCollecting pydantic-core==2.27.0 (from pydantic==2.10)\n  Downloading pydantic_core-2.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic==2.10) (4.12.2)\nDownloading pydantic-2.10.0-py3-none-any.whl (454 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.27.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.23.4\n    Uninstalling pydantic_core-2.23.4:\n      Successfully uninstalled pydantic_core-2.23.4\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.9.2\n    Uninstalling pydantic-2.9.2:\n      Successfully uninstalled pydantic-2.9.2\nSuccessfully installed pydantic-2.10.0 pydantic-core-2.27.0\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install --quiet pyngrok  chainlit langchain_openai langchain langchain_core langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:14:51.166862Z","iopub.execute_input":"2025-01-09T15:14:51.167091Z","iopub.status.idle":"2025-01-09T15:15:30.372672Z","shell.execute_reply.started":"2025-01-09T15:14:51.167071Z","shell.execute_reply":"2025-01-09T15:15:30.371769Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m326.4/326.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.3/60.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.0/345.0 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.2/159.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.8/118.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.4/130.4 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for literalai (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for syncer (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.29.3 which is incompatible.\ncudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 18.1.0 which is incompatible.\ndistributed 2024.8.0 requires dask==2024.8.0, but you have dask 2024.12.1 which is incompatible.\ngoogle-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-bigtable 2.26.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.3 which is incompatible.\npandas-gbq 0.23.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.29.3 which is incompatible.\ntensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.3 which is incompatible.\ntensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.29.3 which is incompatible.\nypy-websocket 0.8.4 requires aiofiles<23,>=22.1.0, but you have aiofiles 23.2.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!ngrok config add-authtoken 2nWCWyomomJ1lyKhW8JYbJpKT1S_2cBhY2ssSnyuDVzbtP7Eu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:15:30.374009Z","iopub.execute_input":"2025-01-09T15:15:30.374359Z","iopub.status.idle":"2025-01-09T15:15:31.518344Z","shell.execute_reply.started":"2025-01-09T15:15:30.374335Z","shell.execute_reply":"2025-01-09T15:15:31.517495Z"}},"outputs":[{"name":"stdout","text":"Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml                                \n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%%writefile interface.py\nimport os\nfrom typing import List\nfrom datetime import datetime\nfrom pipeline import retrieval_legal_documents\nimport chainlit as cl\nfrom langchain_openai import ChatOpenAI\nfrom langchain.agents import AgentExecutor, create_tool_calling_agent\nfrom langchain_core.tools import tool\nfrom langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.callbacks.base import BaseCallbackHandler\n\n# Custom callback handler for streaming\n\nLEGAL_SYSTEM_PROMPT = \"\"\"Bạn là một trợ lý pháp lý chuyên nghiệp, \nchuyên về luật pháp Việt Nam. Nhiệm vụ của bạn là cung cấp thông tin chính xác, \nrõ ràng và hữu ích dựa trên các văn bản pháp luật được cung cấp. Bạn cũng có công cụ để\ntìm tin tức trong trường hợp người dùng yêu cầu. Bạn cũng có công cụ để lấy ngày tháng hiện tại\nhãy sử dụng kết hợp các công cụ để hỗ trợ người dùng\n\nNẾU CÂU HỎI CÓ LIÊN QUAN ĐẾN LUẬT THÌ TRẢ LỜI THEO FORMAT SAU:\nNhiệm vụ chính:\n1. CHỈ trả lời dựa trên ngữ cảnh pháp lý được cung cấp. Không được dùng thông tin ngoài tài liệu được cung cấp.\n2. Đảm bảo tính chính xác về mặt pháp lý.\n3. Duy trì giọng điệu chuyên nghiệp, khách quan.\n4. Sử dụng một cách thông minh các công cụ để hỗ trời người dùng\n\nCấu trúc câu trả lời:\n1. Câu trả lời trực tiếp: Trả lời ngắn gọn\n2. Giải thích chi tiết:\n    * Giải thích rõ ràng, nên ghi chi tiết nhất có thể dựa vào tài liệu bạn có\n    * Ghi ra thật cụ thể thông tin cần thiết từ tài liệu bạn được cung cấp\n    * Phải ghi cụ thể chứ không được trả lời chung chung\n3. Căn cứ pháp lý: Trích dẫn các điều luật liên quan\n4. Lưu ý thêm: Các điểm cần lưu ý nếu có\nLưu ý: Bạn được cung cấp một công cụ để truy vấn tài liệu dựa trên câu hỏi của người dùng\nsẽ có 10 tài liệu được trả về nhưng thường chỉ có 1 hoặc 2 tài liệu có thông tin chính\nxác, do đó hãy xem xét cẩn thận từng tài liệu trước khi trả lời, nếu nhận thấy không\ncó tài liệu nào có thể trả lời được câu hỏi của người dùng hãy bảo họ là bạn không có\nthông tin\n\"\"\"\n\n\nclass StreamHandler(BaseCallbackHandler):\n    def __init__(self, container):\n        self.container = container\n\n    def on_llm_new_token(self, token: str, **kwargs):\n        self.container.write(token)\n\n\n@tool\ndef get_current_day() -> str:\n    \"\"\"\n    Dùng để lấy ngày tháng năm hiện tại dưới dạng dd/mm/yyyy\n    \"\"\"\n    # Get the current date\n    current_date = datetime.now()\n\n    # Format the date as dd/mm/yyyy\n    formatted_date = current_date.strftime(\"%d/%m/%Y\")\n    return formatted_date\n\n@tool\ndef get_news(query: str, date: str) -> str:\n    \"\"\"\n    Dùng để tìm kiếm tin tức. Ví dụ:\n    query: \"Thông tin thời tiết hôm nay?\"\n    date: \"01/01/2003\"\n    \"\"\"\n    if date == \"19/12/2024\":\n        return \"\"\"\n        Ngày 3.11, Ngân hàng Nhà nước giảm nhẹ tỷ giá trung tâm 1 đồng/USD, \n        xuống còn 23.687 đồng/USD. Các ngân hàng tăng giá bán USD lên mức kịch trần.\n        Eximbank bán USD lên 24.870 đồng, mua vào 24.680 - 24.700 đồng. Vietcombank bán\n        USD với giá 24.872 đồng, mua vào 24.562 - 24.592 đồng… Trên thị trường liên\n        ngân hàng, tỷ giá chốt phiên với mức 24.850 đồng/USD, giảm 27 đồng.\n        \"\"\"\n    else:\n        return \"Không có tin tức về chủ đề này\"\n\n@tool\ndef retrieve_documents(query: str) -> str:\n    \"\"\"\n    Sử dụng công cụ này để truy vấn 10 tài liệu luật liên quan nhất đến câu query\n    Ví dụ cách sử dụng:\n    query: Quy định về kinh doanh như thế nào?\n    \"\"\"\n    return retrieval_legal_documents(query)\n\n\n@cl.on_chat_start\nasync def start_chat():\n    \"\"\"Initialize the chat session\"\"\"\n    # Initialize the ChatOpenAI model\n    llm = ChatOpenAI(\n        base_url=\"https://openrouter.ai/api/v1\",\n        api_key= \"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\",\n        model=\"openai/gpt-4o-mini\",\n        #streaming=True,\n        #model_name=\"gpt-4o-mini\"\n    )\n\n    # Create a conversation memory\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        return_messages=True,\n        output_key=\"output\"\n    )\n\n    # Create the prompt with memory\n    prompt = ChatPromptTemplate.from_messages([\n        (\"system\", LEGAL_SYSTEM_PROMPT),\n        MessagesPlaceholder(variable_name=\"chat_history\"),\n        (\"human\", \"{input}\"),\n        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n    ])\n\n    # Create the agent\n    agent = create_tool_calling_agent(\n        llm=llm,\n        tools=[get_current_day, get_news, retrieve_documents],\n        prompt=prompt\n    )\n\n    # Create the agent executor with memory\n    agent_executor = AgentExecutor(\n        agent=agent,\n        tools=[get_current_day, get_news, retrieve_documents],\n        memory=memory,\n        verbose=True,\n        return_intermediate_steps=True,\n    )\n\n    # Store the agent executor in the user session\n    cl.user_session.set(\"agent\", agent_executor)\n\n    await cl.Message(content=\"Xin chào tôi là AI tư vấn luật, tôi có thể giúp gì cho bạn\").send()\n\n\n@cl.on_message\nasync def main(message: cl.Message):\n    \"\"\"Handle incoming messages\"\"\"\n\n    # Get the agent executor from user session\n    agent_executor = cl.user_session.get(\"agent\")\n\n    # Create a message container\n    msg = cl.Message(content=\"\")\n    await msg.send()\n\n    # Run the agent with streaming\n    async for chunk in agent_executor.astream(\n        {\"input\": message.content},\n    ):\n        # Check if 'output' is in the chunk to handle different streaming behaviors\n        if 'output' in chunk:\n            await msg.stream_token(chunk[\"output\"])\n        elif isinstance(chunk, str):\n            await msg.stream_token(chunk)\n\n    await msg.update()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:15:31.519456Z","iopub.execute_input":"2025-01-09T15:15:31.519694Z","iopub.status.idle":"2025-01-09T15:15:31.526608Z","shell.execute_reply.started":"2025-01-09T15:15:31.519675Z","shell.execute_reply":"2025-01-09T15:15:31.525934Z"}},"outputs":[{"name":"stdout","text":"Writing interface.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# import os\n# os.environ[\"OPENAI_API_KEY\"] = \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:15:31.527270Z","iopub.execute_input":"2025-01-09T15:15:31.527546Z","iopub.status.idle":"2025-01-09T15:15:31.543865Z","shell.execute_reply.started":"2025-01-09T15:15:31.527525Z","shell.execute_reply":"2025-01-09T15:15:31.543057Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from pyngrok import ngrok\n\n# Set up ngrok tunnel to the FastAPI server\npublic_url = ngrok.connect(8000)\nprint(f\"Public URL: {public_url}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:15:31.544878Z","iopub.execute_input":"2025-01-09T15:15:31.545182Z","iopub.status.idle":"2025-01-09T15:15:32.045537Z","shell.execute_reply.started":"2025-01-09T15:15:31.545152Z","shell.execute_reply":"2025-01-09T15:15:32.044710Z"}},"outputs":[{"name":"stdout","text":"Public URL: NgrokTunnel: \"https://4467-35-237-228-176.ngrok-free.app\" -> \"http://localhost:8000\"\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!chainlit run interface.py -w","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T15:15:32.046312Z","iopub.execute_input":"2025-01-09T15:15:32.046553Z"}},"outputs":[{"name":"stdout","text":"2025-01-09 15:15:40.090087: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-01-09 15:15:40.338141: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-01-09 15:15:40.407573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nconfig.json: 100%|█████████████████████████████| 777/777 [00:00<00:00, 5.32MB/s]\nmodel.safetensors: 100%|██████████████████████| 540M/540M [00:02<00:00, 228MB/s]\ntokenizer_config.json: 100%|███████████████| 1.17k/1.17k [00:00<00:00, 11.4MB/s]\nvocab.txt: 100%|█████████████████████████████| 895k/895k [00:00<00:00, 14.4MB/s]\nbpe.codes: 100%|███████████████████████████| 1.14M/1.14M [00:00<00:00, 27.9MB/s]\nadded_tokens.json: 100%|██████████████████████| 22.0/22.0 [00:00<00:00, 152kB/s]\nspecial_tokens_map.json: 100%|█████████████████| 167/167 [00:00<00:00, 1.51MB/s]\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at bkai-foundation-models/vietnamese-bi-encoder and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/kaggle/working/pipeline.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  bi_encoder_checkpoint = torch.load(bi_encoder_path)\n/kaggle/working/interface.py:105: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n  memory = ConversationBufferMemory(\n\n\n\u001b[1m> Entering new None chain...\u001b[0m\n\u001b[32;1m\u001b[1;3m\nInvoking: `retrieve_documents` with `{'query': 'Đi xe không đội nón bảo hiểm bị phạt'}`\n\n\n\u001b[0mBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\nBe aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n\u001b[38;5;200m\u001b[1;3mDocument (relevance score: 0.85):\nXử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy định về điều kiện của phương tiện khi tham gia giao thông Xử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy định về điều kiện của phương tiện khi tham gia giao thông 1 . Phạt tiền từ 100.000 đồng đến 200.000 đồng đối với một trong các hành vi vi phạm sau đây : a ) Điều khiển xe không có còi ; đèn soi biển số ; đèn báo hãm ; gương chiếu hậu bên trái người điều khiển hoặc có nhưng không có tác dụng ; b ) ( bị bãi bỏ bởi điểm đ Khoản 36 Điều 2 Nghị định 123 / 2021 / NĐ-CP ) ; c ) Điều khiển xe không có đèn tín hiệu hoặc có nhưng không có tác dụng ; d ) Sử dụng còi không đúng quy chuẩn kỹ thuật cho từng loại xe ; đ ) Điều khiển xe không có bộ phận giảm thanh , giảm khói hoặc có nhưng không bảo đảm quy chuẩn môi trường về khí thải , tiếng ồn ; e ) Điều khiển xe không có đèn chiếu sáng gần , xa hoặc có nhưng không có tác dụng , không đúng tiêu chuẩn thiết kế ; g ) Điều khiển xe không có hệ thống hãm hoặc có nhưng không có tác dụng , không bảo đảm tiêu chuẩn kỹ thuật ; h ) Điều khiển xe lắp đèn chiếu sáng về phía sau xe . ...\n\nDocument (relevance score: 0.70):\n 1 . Phạt tiền từ 100.000 đồng đến 200.000 đồng đối với hành vi điều khiển xe không có đăng ký , không gắn biển số ( đối với loại xe có quy định phải đăng ký và gắn biển số ) . 2 . Phạt tiền từ 300.000 đồng đến 400.000 đồng đối với một trong các hành vi vi phạm sau đây : a ) Điều khiển xe không có hệ thống hãm hoặc có nhưng không có tác dụng ; b ) Điều khiển xe thô sơ chở khách , chở hàng không bảo đảm tiêu chuẩn về tiện nghi và vệ sinh theo quy định của địa phương . \n\nDocument (relevance score: 0.47):\nXử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy tắc giao thông đường bộ 1 . Phạt tiền từ 100.000 đồng đến 200.000 đồng đối với người điều khiển xe thực hiện một trong các hành vi vi phạm sau đây : a ) Không chấp hành hiệu lệnh , chỉ dẫn của biển báo hiệu , vạch kẻ đường , trừ các hành vi vi phạm quy định tại điểm c , điểm đ , điểm e , điểm h khoản 2 ; điểm a , điểm d , điểm g , điểm i , điểm m khoản 3 ; điểm a , điểm b , điểm c , điểm d , điểm e khoản 4 ; khoản 5 ; điểm b khoản 6 ; điểm a , điểm b khoản 7 ; điểm d khoản 8 Điều này ; b ) Không có báo hiệu xin vượt trước khi vượt ; c ) Không giữ khoảng cách an toàn để xảy ra va chạm với xe chạy liền trước hoặc không giữ khoảng cách theo quy định của biển báo hiệu “ Cự ly tối thiểu giữa hai xe ” ; d ) Chuyển hướng không nhường quyền đi trước cho : Người đi bộ , xe lăn của người khuyết tật qua đường tại nơi có vạch kẻ đường dành cho người đi bộ ; xe thô sơ đang đi trên phần đường dành cho xe thô sơ ; đ ) Chuyển hướng không nhường đường cho : Các xe đi ngược chiều ; người đi bộ , xe lăn của người khuyết tật đang qua đường tại nơi không có vạch kẻ đường cho người đi bộ ; e ) Lùi xe mô tô ba bánh không quan sát hoặc không có tín hiệu báo trước ; g ) Chở người ngồi trên xe sử dụng ô ( dù ) ; h ) Không tuân thủ các quy định về nhường đường tại nơi đường giao nhau , trừ các hành vi vi phạm quy định tại điểm b , điểm e khoản 2 Điều này ; ...\n\nDocument (relevance score: 0.38):\n Điều 8 . Xử phạt người điều khiển xe đạp , xe đạp máy ( kể cả xe đạp điện ) , người điều khiển xe thô sơ khác vi phạm quy tắc giao thông đường bộ ... 4 . Phạt tiền từ 400.000 đồng đến 600.000 đồng đối với người điều khiển xe thực hiện một trong các hành vi vi phạm sau đây : a ) Điều khiển xe đi vào đường cao tốc , trừ phương tiện phục vụ việc quản lý , bảo trì đường cao tốc ; b ) Gây tai nạn giao thông không dừng lại , không giữ nguyên hiện trường , bỏ trốn không đến trình báo với cơ quan có thẩm quyền , không tham gia cấp cứu người bị nạn ; c ) Điều khiển xe trên đường mà trong máu hoặc hơi thở có nồng độ cồn vượt quá 80 miligam / 100 mililít máu hoặc vượt quá 0,4 miligam / 1 lít khí thở ; d ) Không chấp hành yêu cầu kiểm tra về nồng độ cồn của người thi hành công vụ . đ ) Người điều khiển xe đạp máy ( kể cả xe đạp điện ) không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách khi tham gia giao thông trên đường bộ ; e ) Chở người ngồi trên xe đạp máy ( kể cả xe đạp điện ) không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách , trừ trường hợp chở người bệnh đi cấp cứu , trẻ em dưới 06 tuổi , áp giải người có hành vi vi phạm pháp luật . ... \n\nDocument (relevance score: 0.33):\n Điều 6 . Xử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy tắc giao thông đường bộ 1 . Phạt tiền từ 100.000 đồng đến 200.000 đồng đối với người điều khiển xe thực hiện một trong các hành vi vi phạm sau đây : a ) Không chấp hành hiệu lệnh , chỉ dẫn của biển báo hiệu , vạch kẻ đường , trừ các hành vi vi phạm quy định tại điểm c , điểm đ , điểm e , điểm h khoản 2 ; điểm a , điểm d , điểm g , điểm i , điểm m khoản 3 ; điểm a , điểm b , điểm c , điểm d , điểm e khoản 4 ; khoản 5 ; điểm b khoản 6 ; điểm a , điểm b khoản 7 ; điểm d khoản 8 Điều này ; b ) Không có báo hiệu xin vượt trước khi vượt ; c ) Không giữ khoảng cách an toàn để xảy ra va chạm với xe chạy liền trước hoặc không giữ khoảng cách theo quy định của biển báo hiệu “ Cự ly tối thiểu giữa hai xe ” ; d ) Chuyển hướng không nhường quyền đi trước cho : Người đi bộ , xe lăn của người khuyết tật qua đường tại nơi có vạch kẻ đường dành cho người đi bộ ; xe thô sơ đang đi trên phần đường dành cho xe thô sơ ; đ ) Chuyển hướng không nhường đường cho : Các xe đi ngược chiều ; người đi bộ , xe lăn của người khuyết tật đang qua đường tại nơi không có vạch kẻ đường cho người đi bộ ; e ) Lùi xe mô tô ba bánh không quan sát hoặc không có tín hiệu báo trước ; g ) Chở người ngồi trên xe sử dụng ô ( dù ) ; h ) Không tuân thủ các quy định về nhường đường tại nơi đường giao nhau , trừ các hành vi vi phạm quy định tại điểm b , điểm e khoản 2 Điều này ; i ) Chuyển làn đường không đúng nơi được phép hoặc không có tín hiệu báo trước ; k ) Điều khiển xe chạy dàn hàng ngang từ 03 xe trở lên ; l ) Không sử dụng đèn chiếu sáng trong thời gian từ 19 giờ ngày hôm trước đến 05 giờ ngày hôm sau hoặc khi sương mù , thời tiết xấu hạn chế tầm nhìn ; m ) Tránh xe không đúng quy định ; sử dụng đèn chiếu xa khi tránh xe đi ngược chiều ; không nhường đường cho xe đi ngược chiều theo quy định tại nơi đường hẹp , đường dốc , nơi có chướng ngại vật ; n ) Bấm còi trong thời gian từ 22 giờ ngày hôm trước đến 05 giờ ngày hôm sau , sử dụng đèn chiếu xa trong đô thị , khu đông dân cư , trừ các xe ưu tiên đang đi làm nhiệm vụ theo quy định ; o ) Xe được quyền ưu tiên lắp đặt , sử dụng thiết bị phát tín hiệu ưu tiên không đúng quy định hoặc sử dụng thiết bị phát tín hiệu ưu tiên mà không có Giấy phép của cơ quan có thẩm quyền cấp hoặc có Giấy phép của cơ quan có thẩm quyền cấp nhưng không còn giá trị sử dụng theo quy định ; p ) Quay đầu xe tại nơi không được quay đầu xe , trừ hành vi vi phạm quy định tại điểm d khoản 4 Điều này ; q ) Điều khiển xe chạy dưới tốc độ tối thiểu trên những đoạn đường bộ có quy định tốc độ tối thiểu cho phép . ... \n\nDocument (relevance score: 0.31):\nTrách nhiệm của người điều khiển , người ngồi trên xe mô tô , xe gắn máy , xe đạp máy Người điều khiển , người ngồi trên xe mô tô , xe gắn máy ( kể cả xe máy điện ) , xe đạp máy khi tham gia giao thông có trách nhiệm : 1 . Đội mũ bảo hiểm theo đúng quy định của pháp luật . 2 . Cài quai mũ theo quy định sau đây : a ) Kéo quai mũ bảo hiểm sang hai bên rồi đội mũ và đóng khóa mũ lại . Không được để quai mũ lỏng lẻo mà phải đóng khít với cằm ; b ) Sau khi đội mũ bảo hiểm cần kiểm tra lại bằng cách : dùng tay kéo mũ từ đằng sau ra đằng trước hoặc nâng phần trên trước trán ( hoặc phần cằm đối với mũ cả hàm ) lên rồi kéo ra đằng sau , mũ không được bật ra khỏi đầu .\n\nDocument (relevance score: 0.27):\n Điều 30 . Xử phạt chủ phương tiện vi phạm quy định liên quan đến giao thông đường bộ ... 5 . Phạt tiền từ 800.000 đồng đến 2.000.000 đồng đối với cá nhân , từ 1.600.000 đồng đến 4.000.000 đồng đối với tổ chức là chủ xe mô tô , xe gắn máy và các loại xe tương tự xe mô tô thực hiện một trong các hành vi vi phạm sau đây : a ) Tự ý cắt , hàn , đục lại số khung , số máy ; đưa phương tiện đã bị cắt , hàn , đục lại số khung , số máy trái quy định tham gia giao thông ; b ) Tẩy xóa , sửa chữa hoặc giả mạo hồ sơ đăng ký xe ; c ) Tự ý thay đổi khung , máy , hình dáng , kích thước , đặc tính của xe ; d ) Khai báo không đúng sự thật hoặc sử dụng các giấy tờ , tài liệu giả để được cấp lại biển số , Giấy đăng ký xe ; đ ) Giao xe hoặc để cho người không đủ điều kiện theo quy định tại khoản 1 Điều 58 của Luật Giao thông đường bộ điều khiển xe tham gia giao thông ( bao gồm cả trường hợp người điều khiển phương tiện có Giấy phép lái xe nhưng đã hết hạn sử dụng hoặc đang trong thời gian bị tước quyền sử dụng ) ; e ) Không chấp hành việc thu hồi Giấy đăng ký xe , biển số xe theo quy định ; g ) Đưa phương tiện không có Giấy đăng ký xe tham gia giao thông hoặc có nhưng đã hết hạn sử dụng ; đưa phương tiện có Giấy chứng nhận đăng ký xe tạm thời , phương tiện có phạm vi hoạt động hạn chế tham gia giao thông quá thời hạn , tuyến đường , phạm vi cho phép h ) Đưa phương tiện có Giấy đăng ký xe nhưng không do cơ quan có thẩm quyền cấp hoặc bị tẩy xóa tham gia giao thông ; đưa phương tiện có Giấy đăng ký xe nhưng không đúng với số khung số máy của xe tham gia giao thông ; i ) Lắp đặt , sử dụng thiết bị thay đổi biển số trên xe trái quy định ; k ) Đưa phương tiện không gắn biển số ( đối với loại xe có quy định phải gắn biển số ) tham gia giao thông ; đưa phương tiện gắn biển số không đúng với Giấy đăng ký xe hoặc gắn biển số không do cơ quan có thẩm quyền cấp tham gia giao thông . ... \n\nDocument (relevance score: 0.26):\n Điều 6 . Xử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy tắc giao thông đường bộ ... 3 . Phạt tiền từ 400.000 đồng đến 600.000 đồng đối với người điều khiển xe thực hiện một trong các hành vi vi phạm sau đây : a ) Chuyển hướng không giảm tốc độ hoặc không có tín hiệu báo hướng rẽ ( trừ trường hợp điều khiển xe đi theo hướng cong của đoạn đường bộ ở nơi đường không giao nhau cùng mức ) ; điều khiển xe rẽ trái tại nơi có biển báo hiệu có nội dung cấm rẽ trái đối với loại phương tiện đang điều khiển ; điều khiển xe rẽ phải tại nơi có biển báo hiệu có nội dung cấm rẽ phải đối với loại phương tiện đang điều khiển b ) Chở theo từ 03 người trở lên trên xe ; c ) Bấm còi , rú ga ( nẹt pô ) liên tục trong đô thị , khu đông dân cư , trừ các xe ưu tiên đang đi làm nhiệm vụ theo quy định ; d ) Dừng xe , đỗ xe trên cầu ; đ ) Điều khiển xe thành đoàn gây cản trở giao thông , trừ trường hợp được cơ quan có thẩm quyền cấp phép ; e ) Điều khiển xe có liên quan trực tiếp đến vụ tai nạn giao thông mà không dừng lại , không giữ nguyên hiện trường , không tham gia cấp cứu người bị nạn , trừ hành vi vi phạm quy định tại điểm đ khoản 8 Điều này ; g ) Điều khiển xe không đi bên phải theo chiều đi của mình ; đi không đúng phần đường , làn đường quy định ( làn cùng chiều hoặc làn ngược chiều ) ; điều khiển xe đi qua dải phân cách cố định ở giữa hai phần đường xe chạy ; điều khiển xe đi trên hè phố , trừ trường hợp điều khiển xe đi qua hè phố để vào nhà ; h ) Vượt bên phải trong trường hợp không được phép ; i ) Đi vào khu vực cấm , đường có biển báo hiệu có nội dung cấm đi vào đối với loại phương tiện đang điều khiển , trừ các hành vi vi phạm quy định tại khoản 5 , điểm b khoản 6 Điều này và các trường hợp xe ưu tiên đang đi làm nhiệm vụ khẩn cấp theo quy định ; k ) Người đang điều khiển xe hoặc chở người ngồi trên xe bám , kéo , đẩy xe khác , vật khác , dẫn dắt súc vật , mang vác vật cồng kềnh ; chở người đứng trên yên , giá đèo hàng hoặc ngồi trên tay lái ; xếp hàng hóa trên xe vượt quá giới hạn quy định ; điều khiển xe kéo theo xe khác , vật khác ; l ) Chở hàng vượt trọng tải thiết kế được ghi trong Giấy đăng ký xe đối với loại xe có quy định về trọng tải thiết kế ; m ) Chạy trong hầm đường bộ không sử dụng đèn chiếu sáng gần . n ) Không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách khi điều khiển xe tham gia giao thông trên đường bộ ; o ) Chở người ngồi trên xe không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách , trừ trường hợp chở người bệnh đi cấp cứu , trẻ em dưới 06 tuổi , áp giải người có hành vi vi phạm pháp luật . ” ... 10 . Ngoài việc bị phạt tiền , người điều khiển xe thực hiện hành vi vi phạm còn bị áp dụng các hình thức xử phạt bổ sung sau đây : a ) Thực hiện hành vi quy định tại điểm g khoản 2 Điều này bị tịch thu thiết bị phát tín hiệu ưu tiên lắp đặt , sử dụng trái quy định ; b ) Thực hiện hành vi quy định tại điểm b , điểm e , điểm i khoản 3 ; điểm e , điểm g , điểm h khoản 4 ; khoản 5 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 01 tháng đến 03 tháng ; c ) Thực hiện hành vi quy định tại điểm a khoản 6 ; điểm a , điểm b khoản 7 ; điểm a , điểm b , điểm c , điểm d khoản 8 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 02 tháng đến 04 tháng ; tái phạm hoặc vi phạm nhiều lần hành vi quy định tại điểm a , điểm b , điểm c , điểm d khoản 8 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 03 tháng đến 05 tháng , tịch thu phương tiện . Thực hiện hành vi quy định tại một trong các điểm , khoản sau của Điều này mà gây tai nạn giao thông thì bị tước quyền sử dụng Giấy phép lái xe từ 02 tháng đến 04 tháng : Điểm a , điểm g , điểm h , điểm k , điểm l , điểm m , điểm n , điểm q khoản 1 ; điểm b , điểm d , điểm e , điểm g , điểm l , điểm m khoản 2 ; điểm b , điểm c , điểm k , điểm m khoản 3 ; điểm đ , điểm e , điểm g , điểm h khoản 4 Điều này ; d ) Thực hiện hành vi quy định tại điểm b khoản 6 ; điểm đ khoản 8 ; khoản 9 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 03 tháng đến 05 tháng ; đ ) Thực hiện hành vi quy định tại điểm c khoản 6 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 10 tháng đến 12 tháng ; e ) Thực hiện hành vi quy định tại điểm c khoản 7 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 16 tháng đến 18 tháng ; g ) Thực hiện hành vi quy định tại điểm e , điểm g , điểm h , điểm i khoản 8 Điều này bị tước quyền sử dụng Giấy phép lái xe từ 22 tháng đến 24 tháng . \n\nDocument (relevance score: 0.25):\nXử phạt người điều khiển xe mô tô , xe gắn máy ( kể cả xe máy điện ) , các loại xe tương tự xe mô tô và các loại xe tương tự xe gắn máy vi phạm quy tắc giao thông đường bộ ... 3 . Phạt tiền từ 400.000 đồng đến 600.000 đồng đối với người điều khiển xe thực hiện một trong các hành vi vi phạm sau đây : a ) Chuyển hướng không giảm tốc độ hoặc không có tín hiệu báo hướng rẽ ( trừ trường hợp điều khiển xe đi theo hướng cong của đoạn đường bộ ở nơi đường không giao nhau cùng mức ) ; điều khiển xe rẽ trái tại nơi có biển báo hiệu có nội dung cấm rẽ trái đối với loại phương tiện đang điều khiển ; điều khiển xe rẽ phải tại nơi có biển báo hiệu có nội dung cấm rẽ phải đối với loại phương tiện đang điều khiển ; b ) Chở theo từ 03 người trở lên trên xe ; c ) Bấm còi , rú ga ( nẹt pô ) liên tục trong đô thị , khu đông dân cư , trừ các xe ưu tiên đang đi làm nhiệm vụ theo quy định ; ... n ) Không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách khi điều khiển xe tham gia giao thông trên đường bộ ; o ) Chở người ngồi trên xe không đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” hoặc đội “ mũ bảo hiểm cho người đi mô tô , xe máy ” không cài quai đúng quy cách , trừ trường hợp chở người bệnh đi cấp cứu , trẻ em dưới 06 tuổi , áp giải người có hành vi vi phạm pháp luật .\n\nDocument (relevance score: 0.24):\n Điều 30 . Người điều khiển , người ngồi trên xe mô tô , xe gắn máy 1 . Người điều khiển xe mô tô hai bánh , xe gắn máy chỉ được chở một người , trừ những trường hợp sau thì được chở tối đa hai người : a ) Chở người bệnh đi cấp cứu ; b ) Áp giải người có hành vi vi phạm pháp luật ; c ) Trẻ em dưới 14 tuổi . 2 . Người điều khiển , người ngồi trên xe mô tô hai bánh , xe mô tô ba bánh , xe gắn máy phải đội mũ bảo hiểm có cài quai đúng quy cách . 3 . Người điều khiển xe mô tô hai bánh , xe mô tô ba bánh , xe gắn máy không được thực hiện các hành vi sau đây : a ) Đi xe dàn hàng ngang ; b ) Đi xe vào phần đường dành cho người đi bộ và phương tiện khác ; c ) Sử dụng ô , điện thoại di động , thiết bị âm thanh , trừ thiết bị trợ thính ; d ) Sử dụng xe để kéo , đẩy xe khác , vật khác , mang , vác và chở vật cồng kềnh ; đ ) Buông cả hai tay hoặc đi xe bằng một bánh đối với xe hai bánh , bằng hai bánh đối với xe ba bánh ; e ) Hành vi khác gây mất trật tự , an toàn giao thông . 4 . Người ngồi trên xe mô tô hai bánh , xe mô tô ba bánh , xe gắn máy khi tham gia giao thông không được thực hiện các hành vi sau đây : a ) Mang , vác vật cồng kềnh ; b ) Sử dụng ô ; c ) Bám , kéo hoặc đẩy các phương tiện khác ; d ) Đứng trên yên , giá đèo hàng hoặc ngồi trên tay lái ; đ ) Hành vi khác gây mất trật tự , an toàn giao thông . \u001b[0m\u001b[32;1m\u001b[1;3m1. **Câu trả lời trực tiếp**: Người điều khiển xe không đội nón bảo hiểm sẽ bị phạt tiền từ 100.000 đồng đến 200.000 đồng.\n\n2. **Giải thích chi tiết**: \n   - Theo quy định tại Nghị định 100/2019/NĐ-CP, người điều khiển xe mô tô, xe gắn máy (bao gồm cả xe máy điện) phải đội mũ bảo hiểm đúng quy cách khi tham gia giao thông. Nếu người điều khiển không đội mũ bảo hiểm hoặc đội không đúng cách, sẽ bị xử phạt tiền từ 100.000 đồng đến 200.000 đồng.\n   - Ngoài việc không đội mũ bảo hiểm, nếu người ngồi trên xe cũng không đội mũ bảo hiểm đúng quy cách, mức phạt cũng tương tự áp dụng cho họ.\n\n3. **Căn cứ pháp lý**: \n   - Nghị định 100/2019/NĐ-CP, Điều 6, khoản 2, điểm n và o quy định về việc xử phạt vi phạm hành chính trong lĩnh vực giao thông đường bộ. \n\n4. **Lưu ý thêm**: \n   - Việc không đội mũ bảo hiểm không chỉ liên quan đến mức phạt mà còn ảnh hưởng đến sự an toàn của người tham gia giao thông. Do đó, việc tuân thủ quy định này là rất quan trọng để bảo vệ sức khỏe và tính mạng.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n\n\n\u001b[1m> Entering new None chain...\u001b[0m\n\u001b[32;1m\u001b[1;3mBạn đã hỏi về việc đi xe không đội nón bảo hiểm bị phạt như thế nào.\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"ngrok.kill()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}