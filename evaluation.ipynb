{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9737512,"sourceType":"datasetVersion","datasetId":5959883},{"sourceId":9823811,"sourceType":"datasetVersion","datasetId":6024014},{"sourceId":10048316,"sourceType":"datasetVersion","datasetId":6190771},{"sourceId":10052283,"sourceType":"datasetVersion","datasetId":6193773},{"sourceId":182747,"sourceType":"modelInstanceVersion","modelInstanceId":155763,"modelId":178220},{"sourceId":182847,"sourceType":"modelInstanceVersion","modelInstanceId":155852,"modelId":178308},{"sourceId":183225,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":156171,"modelId":178621}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Sơ lược về dataset và model\n* Dataset:\n  * **soict-dataset-2024**: Bộ dữ liêu gốc.\n  * **soict-dataset-2024-segmented**: Bộ dữ liêu gốc được segment dùng underthesea\n  * **cross-encoder-dataset**: Dataset này được tạo ra bằng cách lấy ground truth + 3 hard negative trả về bởi bi-encoder + 1 random negative,tập train gồm hơn 500000 sample và tập val có hơn 100000 sample\n  * **cross-encoder-dataset-segmented**: Giống cross-encoder-dataset nhưng được tạo ra bằng bi-encoder mới và có segmentation\n* Mô hình:\n    * **bi_encoder**: Đây là mô hình BiEncoder với base model là  https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder được train trên tập dữ liệu **soict-dataset-2024-segmented** với 2 epoch có mine hard negative\n    * **bi_encoder_embedding** : Đây là FAISS vector database chứa các embedding vector của toàn bộ document trong corpus_segmented.csv được tạo ra bằng cách dùng document_encoder của của bi_encoder\n    * **cross_encoder_new**: Mô hình cross-encoder được tạo ra bằng thư viện sentence_transformer và được train trên tập dữ liệu **soict-dataset-2024-segmented**, dùng để reranking các kết quả trả về từ bi-encoder \n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"../input/cross-encoder-dataset/train_data.csv\")\ndf[:10]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:06:32.400845Z","iopub.execute_input":"2024-11-29T06:06:32.401537Z","iopub.status.idle":"2024-11-29T06:06:48.359983Z","shell.execute_reply.started":"2024-11-29T06:06:32.401502Z","shell.execute_reply":"2024-11-29T06:06:48.359120Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize the dataset for cross-encoder\nfor i in range(len(df[:10])):\n    \n    if df.iloc[i]['label'] == 1:\n        print(\"------------------------------------------------------------\")\n        print(f\" QUERY: {df.iloc[i]['question']}\")\n        print(\" - This is relevant document\")\n        print(df.iloc[i]['document'])\n    elif (df.iloc[i]['label'] == 0 and df.iloc[i+1]['label'] == 1):\n        print(\" - This is random negative\")\n        print(df.iloc[i]['document'])\n    else:\n        print(\" - This is hard negative\")\n        print(df.iloc[i]['document'])\n    \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T06:06:48.361346Z","iopub.execute_input":"2024-11-29T06:06:48.361629Z","iopub.status.idle":"2024-11-29T06:06:48.370441Z","shell.execute_reply.started":"2024-11-29T06:06:48.361602Z","shell.execute_reply":"2024-11-29T06:06:48.369462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Những thứ đã được thực hiện trong notebook này:\n1. Train một mô hình bi-encoder với XML-RoBERTA với 1 epoch **bi_encoder_xlmRoBERTA** , chọn mô hình nàu làm baseline vì nó đã được pretrain bằng nhiều ngôn ngữ gồm tiếng Việt\n2. Sử dụng mô hình bi-encoder vừa train để thực hiện retrieval trên toàn bộ dataset, với mỗi sample trong dataset lọc lấy các ground truth, và chọn 3 hard negative từ kết quả trả về (có thể hiểu ứng với mỗi query chọn 3 kết quả sai được rank cao nhất khi thực hiện retrieval bằng bi-encoder) cùng với 1 random negative, tổng hợp lại ta được một dataset mới là **cross-encoder-dataset** dùng để train cross-encoder (lí do phải tạo dataset mới để train cross-encoder là vì cross-encoder sẽ được dùng để reranking các candidate trả về bởi bi-encoder, mà các candidate là top@k trả về từ bi-encoder do đó rất tương đồng với câu query vì thế phải train cross-encoder trên các hard negative trả về từ bi-encoder.\n3. Sử dụng mô hình **bi_encoder_xlmRoBERTA** trích xuất đặc trưng trên toàn bộ corpus và lưu vào FAISS vector database để lưu trữ và fast silimarity search. Vector embedding được chứa trong **bi_encoder_corpus_embedding**\n4. Định nghĩa một mô hình cross-encoder cơ bản dùng thư viện sentence_transformer với base model là XML-RoBERTA và huấn luyện nó trên bộ dữ liệu **cross-encoder-dataset**, khi huấn luyện nhận thấy kết quả không thay đổi nhiều nên đã dừng train sau khoảng 200000 sample, kết quả thu được mô hình **cross_encoder_ckp** dùng để reranking.\n5. Định nghĩa một pipeline để load các mô hình đã train được để sử dụng và đánh giá hiệu quả mô hình.","metadata":{}},{"cell_type":"markdown","source":"### Một số ý tưởng cách cải thiện mô hình\n1. Cải thiện Bi-encoder:\n   * Train lại bi-encoder dùng base model được train riêng biệt cho tiếng việt như PhoBERT https://huggingface.co/vinai/phobert-base. dùng các thư viện như underthesea để word segmentation dataset trước khi train\n   * Thay vì code chay bi-encoder ta có thể dùng một bi-encoder được pretrain sẵn cho tiếng Việt như https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder, nếu dùng bi-encoder được pretrain cần phải viết lại một hàm mới để load và train mô hình và word segmentation nếu cần thiết\n   * Train thêm nhiều epoch và mine hard negative, mô hình hiện tại dùng XML-RoBERTA và chỉ train trên 1 epoch chưa mine hard negative vì bị tràn ram\n2. Cải thiện Cross-encoder:\n   * Tạo lại dataset mới để train cho cross-encoder từ mô hình bi-encoder đã được cải thiện\n   * Với dataset **cross-encoder-dataset** hiện tại chỉ có 2 class là 1 cho ground truth và 0 cho cả hard và random negative. Ta có thể tạo thêm 1 class nữa với label là 2 cho random negative để giảm sự mất cân bằng dữ liệu.\n   * Tìm thêm các cách tốt hơn để tạo dataset để train cross-encoder.\n   * Thay vì dùng base model là XLM-RoBERTA nên chuyển sang dùng thử PhoBERT.","metadata":{}},{"cell_type":"markdown","source":"### Các thay đổi đã được thực hiện\n1. Sử dụng underthesea để thực hiện word segmentation cho train.csv và corpus.csv, thu được tập dữ liệu **soict-dataset-2024-segmented**\n2. Dùng https://huggingface.co/bkai-foundation-models/vietnamese-bi-encoder để làm base model và train một mô hình bi-encoder mới trên tập dữ **soict-dataset-2024-segmented**, mô hình bi-encoder mới được train trên 2 epochs có mining hard negative và layers freezing, (xem method train của BiEncoderTrainer)\n3. Dùng bi-encoder mới để tạo bộ dữ liệu mới là **cross-encoder-segmented** để train một mô hình cross-encoder mới\n4. Train mô hình cross-encoder mới với base model tương tự như bi-encoder","metadata":{}},{"cell_type":"markdown","source":"**NOTE**: Hiện tại kaggle đang bị lỗi gì đấy nên thi thoảng sẽ không show progress bar hoặc hiển thị javascript error, lỗi này không ảnh hưởng đến việc chạy code nhưng sẽ khá phiền vì đôi lúc không biết mô hình train đến đâu.","metadata":{}},{"cell_type":"markdown","source":"## STAGE 1 - Bi-encoder (First Retrieval Stage)","metadata":{}},{"cell_type":"code","source":"!pip install faiss-gpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:39:25.939231Z","iopub.execute_input":"2024-11-30T04:39:25.939724Z","iopub.status.idle":"2024-11-30T04:39:38.448450Z","shell.execute_reply.started":"2024-11-30T04:39:25.939694Z","shell.execute_reply":"2024-11-30T04:39:38.447406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sentence-transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:39:38.450921Z","iopub.execute_input":"2024-11-30T04:39:38.451287Z","iopub.status.idle":"2024-11-30T04:39:47.423834Z","shell.execute_reply.started":"2024-11-30T04:39:38.451248Z","shell.execute_reply":"2024-11-30T04:39:47.423004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install underthesea","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:39:47.425389Z","iopub.execute_input":"2024-11-30T04:39:47.425890Z","iopub.status.idle":"2024-11-30T04:39:56.909602Z","shell.execute_reply.started":"2024-11-30T04:39:47.425849Z","shell.execute_reply":"2024-11-30T04:39:56.908768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom transformers import XLMRobertaModel, XLMRobertaTokenizer, AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nimport pandas as pd\nfrom tqdm import tqdm\nfrom typing import List, Dict, Tuple, Union\nimport faiss\nimport numpy as np\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:03.380726Z","iopub.execute_input":"2024-11-30T05:08:03.381081Z","iopub.status.idle":"2024-11-30T05:08:03.388879Z","shell.execute_reply.started":"2024-11-30T05:08:03.381036Z","shell.execute_reply":"2024-11-30T05:08:03.387748Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BiEncoderConfig:\n    def __init__(\n        self,\n        max_length: int = 256,\n        batch_size: int = 16,\n        learning_rate: float = 1e-5,\n        num_epochs: int = 2,\n        temperature: float = 0.05,\n        embedding_dim: int = 768,\n\n    ):\n        self.max_length = max_length\n        self.batch_size = batch_size\n        self.learning_rate = learning_rate\n        self.num_epochs = num_epochs\n        self.temperature = temperature\n        self.embedding_dim = embedding_dim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:04.371447Z","iopub.execute_input":"2024-11-30T05:08:04.372044Z","iopub.status.idle":"2024-11-30T05:08:04.376803Z","shell.execute_reply.started":"2024-11-30T05:08:04.372013Z","shell.execute_reply":"2024-11-30T05:08:04.376006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:05.712840Z","iopub.execute_input":"2024-11-30T05:08:05.713474Z","iopub.status.idle":"2024-11-30T05:08:05.718950Z","shell.execute_reply.started":"2024-11-30T05:08:05.713440Z","shell.execute_reply":"2024-11-30T05:08:05.717914Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BiEncoder(nn.Module):\n    def __init__(self, config: BiEncoderConfig):\n        super().__init__()\n        self.config = config\n        \n        # Load the pre-trained Vietnamese bi-encoder for both encoders\n        self.question_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.document_encoder = AutoModel.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n\n        # Use the model's tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n        self.max_length = config.max_length\n\n    def get_device(self):\n        # Helper method to get the current device\n        if isinstance(self.question_encoder, nn.DataParallel):\n            return self.question_encoder.module.device\n        return next(self.question_encoder.parameters()).device\n\n    def mean_pooling(self, model_output, attention_mask):\n        token_embeddings = model_output[0]\n        input_mask_expanded = attention_mask.unsqueeze(\n            -1).expand(token_embeddings.size()).float()\n        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n\n    def encode_question(self, questions: List[str], batch_size: int = 32) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        for i in range(0, len(questions), batch_size):\n            batch_texts = questions[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.question_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n    def encode_document(self, documents: List[str], batch_size: int = 32, disable_progress_bar: bool = False) -> torch.Tensor:\n        all_embeddings = []\n        device = self.get_device()\n\n        # Calculate total number of batches for progress bar\n        num_batches = (len(documents) + batch_size - 1) // batch_size\n\n        for i in tqdm(range(0, len(documents), batch_size), total=num_batches, desc=\"Encoding documents\", disable=disable_progress_bar):\n            batch_texts = documents[i:i + batch_size]\n\n            encoded_input = self.tokenizer(\n                batch_texts,\n                padding=True,\n                truncation=True,\n                max_length=self.max_length,\n                return_tensors='pt'\n            ).to(device)\n\n            with torch.no_grad():\n                model_output = self.document_encoder(**encoded_input)\n\n            batch_embeddings = self.mean_pooling(\n                model_output, encoded_input['attention_mask'])\n            all_embeddings.append(batch_embeddings)\n\n        return torch.cat(all_embeddings, dim=0)\n\n\nclass LegalDataset(Dataset):\n    def __init__(self, questions: List[str], contexts: List[str], tokenizer, max_length: int):\n        self.questions = questions\n        self.contexts = contexts\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        \n    def __len__(self):\n        return len(self.questions)\n    \n    def __getitem__(self, idx):\n        question = self.questions[idx]\n        context = self.contexts[idx]\n        \n        return {\n            'question': question,\n            'context': context\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:06.668171Z","iopub.execute_input":"2024-11-30T05:08:06.668781Z","iopub.status.idle":"2024-11-30T05:08:06.681100Z","shell.execute_reply.started":"2024-11-30T05:08:06.668746Z","shell.execute_reply":"2024-11-30T05:08:06.680294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BiEncoderTrainer:\n\n    def get_config(self) -> BiEncoderConfig:\n        \"\"\"Get the trainer's configuration\"\"\"\n        return self.config\n        \n    def get_model(self) -> BiEncoder:\n        \"\"\"Get the trainer's model\"\"\"\n        return self.model\n        \n    def __init__(self, config: BiEncoderConfig):\n        self.config = config\n        self.model = BiEncoder(self.config)\n        \n        # Initialize with smaller learning rate for fine-tuning\n        self.config.learning_rate = 1e-5  # Reduced from 2e-5\n        \n        # Setup multi-GPU\n        if torch.cuda.device_count() > 1:\n            print(f\"Using {torch.cuda.device_count()} GPUs!\")\n            self.model.question_encoder = nn.DataParallel(self.model.question_encoder)\n            self.model.document_encoder = nn.DataParallel(self.model.document_encoder)\n        \n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.model.to(self.device)\n        \n        # Implement gradual unfreezing\n        self.unfreeze_layers = 0  # Start with all layers frozen\n        self._freeze_layers()\n        \n        # metrics tracking\n        self.best_mrr = 0.0\n        self.best_recall = 0.0\n        \n        # Add new attributes for step-based unfreezing\n        self.total_steps = 0\n        self.unfreeze_schedule = None  # Will be set in train()\n\n    def _freeze_layers(self):\n        \"\"\"Freeze/unfreeze layers gradually during training\"\"\"\n        # First freeze all layers\n        for param in self.model.question_encoder.parameters():\n            param.requires_grad = False\n        for param in self.model.document_encoder.parameters():\n            param.requires_grad = False\n            \n        def unfreeze_model_layers(model, num_layers):\n            # Always unfreeze the pooler and final layer\n            if isinstance(model, nn.DataParallel):\n                model = model.module\n            \n            # Unfreeze pooler\n            for param in model.pooler.parameters():\n                param.requires_grad = True\n            \n            # Always keep the final layer unfrozen\n            if num_layers == 0:\n                for param in model.encoder.layer[-1].parameters():\n                    param.requires_grad = True\n                return\n                \n            # Unfreeze specified number of layers from the top\n            for layer in list(model.encoder.layer)[-num_layers:]:\n                for param in layer.parameters():\n                    param.requires_grad = True\n                \n        # Apply unfreezing to both encoders\n        unfreeze_model_layers(self.model.question_encoder, self.unfreeze_layers)\n        unfreeze_model_layers(self.model.document_encoder, self.unfreeze_layers)\n\n    def set_scores(self, scores: Tuple):\n        self.best_mrr = scores[0]\n        self.best_recall = scores[1]\n        \n    def prepare_batch(self, batch: Dict[str, List[str]]) -> Dict[str, torch.Tensor]:\n        # tokenize questions\n        questions_tokenized = self.model.tokenizer(\n            batch['question'],\n            padding=True,\n            truncation=True,\n            max_length=self.config.max_length,\n            return_tensors='pt'\n        ).to(self.device)\n        \n        #Tokenize contexts\n        contexts_tokenized = self.model.tokenizer(\n            batch['context'],\n            padding=True,\n            truncation=True,\n            max_length=self.config.max_length,\n            return_tensors='pt'\n        ).to(self.device)\n        \n        return {\n            'question_data': questions_tokenized,\n            'context_data': contexts_tokenized\n        }\n    \n    def compute_loss(self, q_embeddings: torch.Tensor, d_embeddings: torch.Tensor,\n                    hard_negative_embeddings: torch.Tensor = None) -> torch.Tensor:\n        \"\"\"Compute loss with both in-batch and hard negatives\"\"\"\n        #regular in-batch negative loss\n        similarity = torch.matmul(q_embeddings, d_embeddings.t())\n        \n        if hard_negative_embeddings is not None:\n            # hard negative similarities\n            hard_similarity = torch.matmul(q_embeddings, hard_negative_embeddings.t())\n            similarity = torch.cat([similarity, hard_similarity], dim=1)\n        \n        # scale by temperature\n        similarity = similarity / self.config.temperature\n        \n        # Create labels for diagonal (positive pairs)\n        labels = torch.arange(q_embeddings.size(0)).to(self.device)\n        \n        # Compute loss\n        loss = nn.CrossEntropyLoss()(similarity, labels)\n        \n        return loss\n    \n    def evaluate(self, val_dataset: LegalDataset) -> Dict[str, float]:\n        \"\"\"\n        Evaluate the model on validation dataset\n        Returns MRR@k and Recall@k metrics\n        \"\"\"\n        self.model.eval()\n        val_dataloader = DataLoader(\n            val_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=False\n        )\n        \n        all_q_embeddings = []\n        all_d_embeddings = []\n        \n        with torch.no_grad():\n            for batch in tqdm(val_dataloader, desc=\"Evaluating\"):\n                processed_batch = self.prepare_batch(batch)\n                \n                # Get embeddings\n                q_embeddings = self.model.mean_pooling(\n                    self.model.question_encoder(**processed_batch['question_data']),\n                    processed_batch['question_data']['attention_mask']\n                )\n                d_embeddings = self.model.mean_pooling(\n                    self.model.document_encoder(**processed_batch['context_data']),\n                    processed_batch['context_data']['attention_mask']\n                )\n                \n                # Normalize\n                q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n                d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n                \n                all_q_embeddings.append(q_embeddings)\n                all_d_embeddings.append(d_embeddings)\n        \n        # Concatenate all embeddings\n        all_q_embeddings = torch.cat(all_q_embeddings, dim=0)\n        all_d_embeddings = torch.cat(all_d_embeddings, dim=0)\n        \n        # Compute similarity matrix\n        similarity = torch.matmul(all_q_embeddings, all_d_embeddings.t())\n        \n        # Calculate metrics\n        k_values = [1, 5, 10, 50,100, 200, 500, 1000]\n        metrics = {}\n        \n        for k in k_values:\n            # Get top-k indices\n            _, indices = similarity.topk(k, dim=1)\n            \n            # Calculate Recall@k\n            correct = torch.arange(similarity.size(0)).unsqueeze(1).expand_as(indices).to(self.device)\n            recall_at_k = (indices == correct).float().sum(dim=1).mean().item()\n            metrics[f'recall@{k}'] = recall_at_k\n            \n            # Calculate MRR@k\n            rank = (indices == correct).nonzero()[:, 1] + 1\n            mrr = (1.0 / rank).mean().item()\n            metrics[f'mrr@{k}'] = mrr\n            \n        return metrics\n\n    def train(self, train_dataset: LegalDataset, val_dataset: LegalDataset = None):\n        train_dataloader = DataLoader(\n            train_dataset,\n            batch_size=self.config.batch_size,\n            shuffle=True\n        )\n        \n        # Calculate total steps and set unfreeze schedule\n        total_steps = len(train_dataloader) * self.config.num_epochs\n        steps_per_epoch = len(train_dataloader)\n        \n        # Adjust unfreeze schedule to be more frequent within the epoch\n        self.unfreeze_schedule = {\n            steps_per_epoch // 4: 2,     # Unfreeze top 2 layers after 25% steps\n            steps_per_epoch // 2: 4,     # Unfreeze top 4 layers after 50% steps\n            3 * steps_per_epoch // 4: 6,  # Unfreeze top 6 layers after 75% steps\n            9 * steps_per_epoch // 10: 8  # Unfreeze top 8 layers after 90% steps\n        }\n        \n        # Use different optimizers for frozen/unfrozen parameters\n        def get_optimizer():\n            params = []\n            for model in [self.model.question_encoder, self.model.document_encoder]:\n                params.extend([p for p in model.parameters() if p.requires_grad])\n            return torch.optim.AdamW(\n                params,\n                lr=self.config.learning_rate,\n                weight_decay=0.01\n            )\n        \n        optimizer = get_optimizer()\n        \n        # Add learning rate scheduler\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            optimizer, \n            T_max=total_steps\n        )\n        \n        # Adjust mining frequency to occur multiple times within epoch\n        mine_every_n_steps = steps_per_epoch // 2  # Mine 4 times per epoch\n        \n        for epoch in range(self.config.num_epochs):\n            self.model.train()\n            total_loss = 0\n            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{self.config.num_epochs}')\n            \n            hard_negatives = None\n            current_epoch_step = 0\n            \n            for batch_idx, batch in enumerate(progress_bar):\n                current_epoch_step = batch_idx\n                \n                # Check if it's time to mine hard negatives\n                if current_epoch_step % mine_every_n_steps == 0 and current_epoch_step != 0:\n                    print(f\"\\nMining hard negatives at step {current_epoch_step}...\")\n                    questions = train_dataset.questions\n                    contexts = train_dataset.contexts\n                    hard_negatives = self.mine_hard_negatives(questions, contexts)\n                \n                # Check unfreeze schedule based on current epoch step\n                if current_epoch_step in self.unfreeze_schedule:\n                    print(f\"\\nUnfreezing layers at step {current_epoch_step}...\")\n                    self.unfreeze_layers = self.unfreeze_schedule[current_epoch_step]\n                    self._freeze_layers()\n                    optimizer = get_optimizer()  # Reinitialize optimizer with new trainable params\n                \n                optimizer.zero_grad()\n                \n                #Prepare batch data\n                processed_batch = self.prepare_batch(batch)\n                \n                #Get embeddings from separate encoders\n                q_embeddings = self.model.mean_pooling(\n                    self.model.question_encoder(**processed_batch['question_data']),\n                    processed_batch['question_data']['attention_mask']\n                )\n                d_embeddings = self.model.mean_pooling(\n                    self.model.document_encoder(**processed_batch['context_data']),\n                    processed_batch['context_data']['attention_mask']\n                )\n                \n                #Process hard negatives if available\n                hard_negative_embeddings = None\n                if hard_negatives is not None:\n                    hard_negative_batch = self.model.tokenizer(\n                        hard_negatives[batch_idx:batch_idx + len(batch)],\n                        padding=True,\n                        truncation=True,\n                        max_length=self.config.max_length,\n                        return_tensors='pt'\n                    ).to(self.device)\n                    \n                    hard_negative_embeddings = self.model.mean_pooling(\n                        self.model.document_encoder(**hard_negative_batch),\n                        hard_negative_batch['attention_mask']\n                    )\n                    hard_negative_embeddings = nn.functional.normalize(hard_negative_embeddings, p=2, dim=1)\n                \n                #Normalize embeddings\n                q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n                d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n                \n                #Compute loss with hard negatives\n                loss = self.compute_loss(q_embeddings, d_embeddings, hard_negative_embeddings)\n                \n                #Backward pass\n                loss.backward()\n                optimizer.step()\n                \n                total_loss += loss.item()\n                progress_bar.set_postfix({'loss': total_loss / (progress_bar.n + 1)})\n            \n            avg_loss = total_loss / len(train_dataloader)\n            print(f'Epoch {epoch + 1}/{self.config.num_epochs}, Average Loss: {avg_loss:.4f}')\n            \n            # Validation step\n            if val_dataset is not None:\n                metrics = self.evaluate(val_dataset)\n                print(\"Validation metrics:\")\n                for metric_name, value in metrics.items():\n                    print(f\"{metric_name}: {value:.4f}\")\n                \n                # Save best model\n                if metrics['mrr@10'] > self.best_mrr:\n                    self.best_mrr = metrics['mrr@10']\n                    self.save_model('best_model.pt')\n    \n    def save_model(self, path: str):\n        # Save both encoders and config\n        torch.save({\n            'question_encoder': self.model.question_encoder.state_dict(),\n            'document_encoder': self.model.document_encoder.state_dict(),\n            'config': self.config\n        }, path)\n    \n    def load_model(self, path: str):\n        checkpoint = torch.load(path)\n        print\n        question_state_dict = checkpoint['question_encoder']\n        document_state_dict = checkpoint['document_encoder']\n        \n        # Remove 'module.' prefix if it exists and model is not using DataParallel\n        if not isinstance(self.model.question_encoder, nn.DataParallel):\n            print(\"remove module.\")\n            question_state_dict = {k.replace('module.', ''): v for k, v in question_state_dict}\n            document_state_dict = {k.replace('module.', ''): v for k, v in document_state_dict}\n        # Add 'module.' prefix if model is using DataParallel but saved model wasn't\n        elif not any(k.startswith('module.') for k in question_state_dict):\n            print(\"add module.\")\n            question_state_dict = {'module.' + k: v for k, v in question_state_dict}\n            document_state_dict = {'module.' + k: v for k, v in document_state_dict}\n        \n        # Load the state dictionaries\n        try:\n            self.model.question_encoder.load_state_dict(question_state_dict)\n            self.model.document_encoder.load_state_dict(document_state_dict)\n        except RuntimeError as e:\n            print(f\"Error loading state dict: {e}\")\n            print(\"Attempting alternative loading method...\")\n            \n            # If the first attempt fails, try the opposite approach\n            if isinstance(self.model.question_encoder, nn.DataParallel):\n                question_state_dict = {k.replace('module.', ''): v for k in question_state_dict}\n                document_state_dict = {k.replace('module.', ''): v for k in document_state_dict}\n            else:\n                question_state_dict = {'module.' + k: v for k in question_state_dict}\n                document_state_dict = {'module.' + k: v for k in document_state_dict}\n            \n            self.model.question_encoder.load_state_dict(question_state_dict)\n            self.model.document_encoder.load_state_dict(document_state_dict)\n\n    def mine_hard_negatives(self, questions: List[str], documents: List[str], \n                       batch_size: int = 256) -> List[str]:  # Reduced batch size\n        \"\"\"Mine hard negatives using embeddings similarity\"\"\"\n        self.model.eval()\n        device = self.device\n    \n        # Significantly reduced chunk sizes to avoid OOM\n        chunk_size = 2000  # Reduced from 5000\n        similarity_chunk_size = 200  # Reduced from 500\n    \n        with torch.no_grad():\n            # Process questions in smaller chunks\n            all_q_embeddings = []\n            for i in tqdm(range(0, len(questions), chunk_size), desc=\"Encoding questions\"):\n                q_chunk = questions[i:i + chunk_size]\n                q_emb = self.model.encode_question(q_chunk, batch_size)\n                all_q_embeddings.append(q_emb.cpu())  # Move to CPU after processing\n                torch.cuda.empty_cache()\n            q_embeddings = torch.cat(all_q_embeddings, dim=0)\n        \n            # Process documents in smaller chunks\n            all_d_embeddings = []\n            for i in tqdm(range(0, len(documents), chunk_size), desc=\"Encoding documents\"):\n                d_chunk = documents[i:i + chunk_size]\n                d_emb = self.model.encode_document(d_chunk, batch_size)\n                all_d_embeddings.append(d_emb.cpu())  # Move to CPU after processing\n                torch.cuda.empty_cache()\n            d_embeddings = torch.cat(all_d_embeddings, dim=0)\n            \n            # Normalize embeddings (on CPU to save GPU memory)\n            q_embeddings = nn.functional.normalize(q_embeddings, p=2, dim=1)\n            d_embeddings = nn.functional.normalize(d_embeddings, p=2, dim=1)\n            \n            # Compute similarity in smaller chunks\n            hard_negative_indices = []\n            k = 2  # Reduced number of hard negatives per question\n            \n            for i in tqdm(range(0, len(q_embeddings), similarity_chunk_size), desc=\"Mining negatives\"):\n                # Move only the current chunks to GPU\n                q_chunk = q_embeddings[i:i + similarity_chunk_size].to(device)\n                d_chunk = d_embeddings.to(device)\n                \n                similarity = torch.matmul(q_chunk, d_chunk.t())\n                \n                # Get top-k most similar but incorrect documents\n                values, indices = similarity.topk(k + 1, dim=1)\n                \n                # Filter out positive pairs\n                mask = torch.arange(i, min(i + similarity_chunk_size, len(q_embeddings))).unsqueeze(1).expand_as(indices).to(device)\n                chunk_negative_indices = indices[indices != mask].view(-1)\n                hard_negative_indices.extend(chunk_negative_indices.cpu().numpy())\n                \n                # Free memory\n                del similarity, values, indices, q_chunk, d_chunk\n                torch.cuda.empty_cache()\n        \n        return [documents[idx] for idx in hard_negative_indices]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:08.435692Z","iopub.execute_input":"2024-11-30T05:08:08.435946Z","iopub.status.idle":"2024-11-30T05:08:08.483328Z","shell.execute_reply.started":"2024-11-30T05:08:08.435916Z","shell.execute_reply":"2024-11-30T05:08:08.482156Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import pandas as pd\n\n# original_df = pd.read_csv(\"../input/soict-dataset-2024-segmented/train_segmented.csv\")\n# original_df.head(), original_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T15:01:37.112531Z","iopub.execute_input":"2024-11-29T15:01:37.113574Z","iopub.status.idle":"2024-11-29T15:01:43.378697Z","shell.execute_reply.started":"2024-11-29T15:01:37.113534Z","shell.execute_reply":"2024-11-29T15:01:43.377526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n\n# train_set, test_set = train_test_split(original_df, test_size=0.1, random_state=42)\n# train_set.shape, test_set.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T15:01:43.380775Z","iopub.execute_input":"2024-11-29T15:01:43.381236Z","iopub.status.idle":"2024-11-29T15:01:43.425003Z","shell.execute_reply.started":"2024-11-29T15:01:43.381188Z","shell.execute_reply":"2024-11-29T15:01:43.423965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Create datasets\n# tokenizer = AutoTokenizer.from_pretrained(\"bkai-foundation-models/vietnamese-bi-encoder\")\n\n# train_dataset = LegalDataset(\n#         questions=train_set['question'].tolist(),\n#         contexts=train_set['context'].tolist(),\n#         tokenizer=tokenizer,\n#         max_length=432\n# )\n    \n# val_dataset = LegalDataset(\n#         questions=test_set['question'].tolist(),\n#         contexts=test_set['context'].tolist(),\n#         tokenizer=tokenizer,\n#         max_length=432\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T15:01:43.426429Z","iopub.execute_input":"2024-11-29T15:01:43.426840Z","iopub.status.idle":"2024-11-29T15:01:43.704601Z","shell.execute_reply.started":"2024-11-29T15:01:43.426796Z","shell.execute_reply":"2024-11-29T15:01:43.703425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# #Initialize config\n# config = BiEncoderConfig(\n#     max_length=256,\n#     batch_size=16,\n#     learning_rate=1e-5,\n#     num_epochs=2,\n#     temperature=0.05\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:20:50.918584Z","iopub.execute_input":"2024-11-29T14:20:50.919257Z","iopub.status.idle":"2024-11-29T14:20:50.923348Z","shell.execute_reply.started":"2024-11-29T14:20:50.919221Z","shell.execute_reply":"2024-11-29T14:20:50.922467Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize trainer\n# trainer = BiEncoderTrainer(config)\n    \n# # Train model with validation\n# trainer.train(train_dataset, val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T12:52:09.935457Z","iopub.execute_input":"2024-11-29T12:52:09.935839Z","iopub.status.idle":"2024-11-29T12:52:09.940099Z","shell.execute_reply.started":"2024-11-29T12:52:09.935808Z","shell.execute_reply":"2024-11-29T12:52:09.939208Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize trainer\n#base_model = BiEncoderTrainer(config)\n#base_model.load_model('best_model.pt')\n#base_model.evaluate(val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T14:12:45.050884Z","iopub.execute_input":"2024-11-28T14:12:45.051111Z","iopub.status.idle":"2024-11-28T14:12:45.059954Z","shell.execute_reply.started":"2024-11-28T14:12:45.051088Z","shell.execute_reply":"2024-11-28T14:12:45.059244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Initialize model path\n# model_path = \"../input/bi-encoder-xlm-roberta/pytorch/default/1/best_model.pt\"\n\n\n# trained_model = BiEncoderTrainer(config)\n# trained_model.load_model(model_path)\n# trained_model.evaluate(val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T14:12:45.060918Z","iopub.execute_input":"2024-11-28T14:12:45.061233Z","iopub.status.idle":"2024-11-28T14:12:45.071251Z","shell.execute_reply.started":"2024-11-28T14:12:45.061199Z","shell.execute_reply":"2024-11-28T14:12:45.070569Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## STAGE 2 - Cross-encoder (Reranking Stage):","metadata":{}},{"cell_type":"code","source":"from sentence_transformers import CrossEncoder\nfrom torch.utils.data import Dataset, DataLoader\nfrom typing import List, Dict\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sentence_transformers.readers import InputExample\nfrom sentence_transformers.cross_encoder.evaluation import CEBinaryAccuracyEvaluator\nfrom sentence_transformers import LoggingHandler\nimport logging\nimport pandas as pd\n\n\nclass LoggingCallback:\n    def __init__(self):\n        self.current_step = 0\n\n    def __call__(self, score: float, epoch: int, steps: int):\n        self.current_step += 1\n        print(f'Step: {self.current_step}, Epoch: {epoch}, Loss: {score:.4f}')\n\n\nclass CrossEncoderWrapper:\n    def __init__(\n        self,\n        model_name: str = \"bkai-foundation-models/vietnamese-bi-encoder\",\n        max_length: int = 256,\n        batch_size: int = 16,\n    ):\n        self.model_name = model_name\n        self.batch_size = batch_size\n        self.max_length = max_length\n        \n        # Explicitly set device\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Initialize model with explicit device\n        self.model = CrossEncoder(\n            model_name,\n            max_length=max_length,\n            device=self.device\n        )\n\n    def train(self, train_dataset: Dataset, val_dataset: Dataset = None,\n              num_epochs: int = 1, warmup_ratio: float = 0.02):\n        \"\"\"Train the cross-encoder model\"\"\"\n\n       # Setup logging\n        logging.basicConfig(format='%(asctime)s - %(message)s',\n                            datefmt='%Y-%m-%d %H:%M:%S',\n                            level=logging.INFO,\n                            handlers=[LoggingHandler()])\n\n        # Prepare training examples\n        train_samples = [\n            InputExample(texts=[sample['question'], sample['document']],\n                         label=sample['label'])\n            for sample in train_dataset\n        ]\n\n        # Create data loader\n        train_dataloader = DataLoader(\n            train_samples,\n            shuffle=True,\n            batch_size=self.batch_size\n        )\n\n        # Create evaluator for validation\n        if val_dataset is not None:\n            # Prepare validation data in the required format\n            val_samples = [\n                InputExample(texts=[sample['question'], sample['document']],\n                             label=sample['label'])\n                for sample in val_dataset\n            ]\n\n            evaluator = CEBinaryAccuracyEvaluator.from_input_examples(\n                val_samples, name=\"evaluation\")\n        else:\n            evaluator = None\n        # Create callback\n        callback = LoggingCallback()\n\n        # Train the model\n        self.model.fit(\n            train_dataloader=train_dataloader,\n            evaluator=evaluator,\n            epochs=num_epochs,\n            evaluation_steps=2000,  # Evaluate every 2000 steps\n            warmup_steps=int(len(train_dataset) * warmup_ratio),\n            show_progress_bar=True,\n            output_path='checkpoints',  # Save checkpoints\n            # save_best_model=True,\n            callback=callback  # Add the callback\n        )\n\n    def evaluate(self, dataset: Dataset) -> Dict[str, float]:\n        \"\"\"Evaluate the model on a dataset\"\"\"\n        pairs = [\n            [sample['question'], sample['document']]\n            for sample in dataset\n        ]\n        labels = [sample['label'] for sample in dataset]\n\n        # Get predictions\n        scores = self.model.predict(pairs)\n        predictions = (scores > 0.5).astype(int)\n\n        # Calculate metrics\n        metrics = {\n            'accuracy': accuracy_score(labels, predictions),\n            'precision': precision_score(labels, predictions),\n            'recall': recall_score(labels, predictions),\n            'f1': f1_score(labels, predictions),\n            'auc_roc': roc_auc_score(labels, scores)\n        }\n\n        return metrics\n\n    def predict(self, questions: List[str], documents: List[str], show_progress_bar=True) -> np.ndarray:\n        \"\"\"Get relevance scores for question-document pairs\"\"\"\n        try:\n            # Ensure model is in eval mode\n            self.model.model.eval()\n            \n            # Create pairs\n            pairs = [[q, d] for q, d in zip(questions, documents)]\n\n            # Make prediction with smaller batch size\n            return self.model.predict(\n                pairs,\n                batch_size=16,  # Smaller batch size\n                show_progress_bar=show_progress_bar\n            )\n            \n        except Exception as e:\n            print(f\"Error during prediction: {str(e)}\")\n            raise\n\n    def save_model(self, path: str):\n        self.model.save(path)\n\n    def load_model(self, path: str):\n        self.model = CrossEncoder(\n            path,\n            max_length=self.max_length,\n            device=self.device\n        )\n\n\nclass CrossEncoderDataset(Dataset):\n    def __init__(self, questions: List[str], documents: List[str], labels: List[int]):\n        self.questions = questions\n        self.documents = documents\n        self.labels = labels\n        assert len(questions) == len(documents) == len(\n            labels), \"All inputs must have the same length\"\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, idx):\n        return {\n            'question': self.questions[idx],\n            'document': self.documents[idx],\n            'label': self.labels[idx]\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:03.490194Z","iopub.execute_input":"2024-11-30T04:40:03.490626Z","iopub.status.idle":"2024-11-30T04:40:17.306542Z","shell.execute_reply.started":"2024-11-30T04:40:03.490587Z","shell.execute_reply":"2024-11-30T04:40:17.305657Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#This is to load saved dataset (the result of bi-encoder)\ndf = pd.read_csv(\"../input/cross-encoder-dataset-segmented/train_data.csv\")\ndf = df.drop_duplicates()\ndf = df.sample(frac=1)\ndf.drop(columns=['cid', 'Unnamed: 0'], inplace=True)\n\ndf.head(), df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:17.307645Z","iopub.execute_input":"2024-11-30T04:40:17.308251Z","iopub.status.idle":"2024-11-30T04:40:29.123157Z","shell.execute_reply.started":"2024-11-30T04:40:17.308213Z","shell.execute_reply":"2024-11-30T04:40:29.122285Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We using most of the dataset for train because 10000 val sample is enough\ntrain_data = df[:230000]\nval_data = df[230000:240000]\ntest_data = df[240000:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:29.124741Z","iopub.execute_input":"2024-11-30T04:40:29.125012Z","iopub.status.idle":"2024-11-30T04:40:29.129396Z","shell.execute_reply.started":"2024-11-30T04:40:29.124987Z","shell.execute_reply":"2024-11-30T04:40:29.128550Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_data # Show the data sample","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:29.130408Z","iopub.execute_input":"2024-11-30T04:40:29.130640Z","iopub.status.idle":"2024-11-30T04:40:29.362996Z","shell.execute_reply.started":"2024-11-30T04:40:29.130617Z","shell.execute_reply":"2024-11-30T04:40:29.362144Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create train, val and test dataset\ntrain_dataset = CrossEncoderDataset(\n        questions=train_data['question'].tolist(),\n        documents=train_data['document'].tolist(),\n        labels=train_data['label'].tolist(),\n    )\n\nval_dataset = CrossEncoderDataset(\n        questions=val_data['question'].tolist(),\n        documents=val_data['document'].tolist(),\n        labels=val_data['label'].tolist(),\n    )\n\ntest_dataset = CrossEncoderDataset(\n        questions=test_data['question'].tolist(),\n        documents=test_data['document'].tolist(),\n        labels=test_data['label'].tolist(),\n    )\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:29.364026Z","iopub.execute_input":"2024-11-30T04:40:29.364424Z","iopub.status.idle":"2024-11-30T04:40:29.388815Z","shell.execute_reply.started":"2024-11-30T04:40:29.364385Z","shell.execute_reply":"2024-11-30T04:40:29.387888Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's see what a data sample look like\nval_dataset[4]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:29.389819Z","iopub.execute_input":"2024-11-30T04:40:29.390304Z","iopub.status.idle":"2024-11-30T04:40:29.402288Z","shell.execute_reply.started":"2024-11-30T04:40:29.390274Z","shell.execute_reply":"2024-11-30T04:40:29.401459Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the encoder (this might show javascript error but ignore it)\nmodel = CrossEncoderWrapper(\n    model_name=\"bkai-foundation-models/vietnamese-bi-encoder\",\n    max_length=256,\n    batch_size=16\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:29.403425Z","iopub.execute_input":"2024-11-30T04:40:29.403772Z","iopub.status.idle":"2024-11-30T04:40:33.647638Z","shell.execute_reply.started":"2024-11-30T04:40:29.403735Z","shell.execute_reply":"2024-11-30T04:40:33.646880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's take a small section of the val dataset and evaluate them to see how an untrained\n# cross-enncoder perform\nval_query = [val_dataset[i]['question'] for i in range(100)]\nval_documents = [val_dataset[i]['document'] for i in range(100)]\nval_labels = [val_dataset[i]['label']for i in range(100)]\nlen(val_query), len(val_documents), len(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:33.649802Z","iopub.execute_input":"2024-11-30T04:40:33.650095Z","iopub.status.idle":"2024-11-30T04:40:33.657135Z","shell.execute_reply.started":"2024-11-30T04:40:33.650067Z","shell.execute_reply":"2024-11-30T04:40:33.656262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import logging\nlogging.disable(logging.WARNING)\n\n# Evaluate untrained cross-encoder\nscores = model.predict(val_query, val_documents)\nscores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:33.658332Z","iopub.execute_input":"2024-11-30T04:40:33.658619Z","iopub.status.idle":"2024-11-30T04:40:35.809930Z","shell.execute_reply.started":"2024-11-30T04:40:33.658593Z","shell.execute_reply":"2024-11-30T04:40:35.809028Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# The prediction will usually be all 0 or all 1 \npredictions = (scores > 0.5).astype(int)\npredictions, np.array(val_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:47.639729Z","iopub.execute_input":"2024-11-30T04:40:47.640384Z","iopub.status.idle":"2024-11-30T04:40:47.646887Z","shell.execute_reply.started":"2024-11-30T04:40:47.640336Z","shell.execute_reply":"2024-11-30T04:40:47.646007Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let't evaluate the model to see how they perform, here the accuracy is 0.82 because\n# 80% of the dataset is labled 0 \nmetrics = {\n            'accuracy': accuracy_score(val_labels, predictions),\n            'precision': precision_score(val_labels, predictions),\n            'recall': recall_score(val_labels, predictions),\n            'f1': f1_score(val_labels, predictions),\n            'auc_roc': roc_auc_score(val_labels, scores)\n        }\nmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:40:48.423769Z","iopub.execute_input":"2024-11-30T04:40:48.424483Z","iopub.status.idle":"2024-11-30T04:40:48.446384Z","shell.execute_reply.started":"2024-11-30T04:40:48.424431Z","shell.execute_reply":"2024-11-30T04:40:48.445581Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# logging.disable(logging.WARNING)\n\n# # Initialize\n# model = CrossEncoderWrapper(\n#     model_name=\"bkai-foundation-models/vietnamese-bi-encoder\",\n#     max_length=256,\n#     batch_size=16\n# )\n\n# # Train\n# model.train(train_dataset, val_dataset, num_epochs=1)\n\n# # Evaluate\n# metrics = model.evaluate(val_dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T00:34:04.570739Z","iopub.execute_input":"2024-11-30T00:34:04.571464Z","iopub.status.idle":"2024-11-30T03:51:34.046170Z","shell.execute_reply.started":"2024-11-30T00:34:04.571410Z","shell.execute_reply":"2024-11-30T03:51:34.045456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's load test dataset\ntest_query = [test_dataset[i]['question'] for i in range(len(test_dataset))]\ntest_documents = [test_dataset[i]['document'] for i in range(len(test_dataset))]\ntest_labels = [test_dataset[i]['label']for i in range(len(test_dataset))]\nlen(test_query), len(test_documents), len(test_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:41:01.900279Z","iopub.execute_input":"2024-11-30T04:41:01.901054Z","iopub.status.idle":"2024-11-30T04:41:01.933565Z","shell.execute_reply.started":"2024-11-30T04:41:01.901019Z","shell.execute_reply":"2024-11-30T04:41:01.932891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = CrossEncoderWrapper(\n    model_name=\"bkai-foundation-models/vietnamese-bi-encoder\",\n    max_length=256,\n    batch_size=16\n)\n\n# Load trained model and predict on 2000 sample of test dataset\nmodel.load_model('/kaggle/input/cross_encoder_new/pytorch/default/1')\nscores = model.predict(test_query[:2000], test_documents[:2000])\n\n# show 100 first prediction\nscores[:100]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:41:50.717852Z","iopub.execute_input":"2024-11-30T04:41:50.718528Z","iopub.status.idle":"2024-11-30T04:42:14.567512Z","shell.execute_reply.started":"2024-11-30T04:41:50.718471Z","shell.execute_reply":"2024-11-30T04:42:14.566749Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Some example prediction\npredictions = (scores > 0.5).astype(int)\npredictions[0:20], np.array(test_labels)[0:20]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:42:19.570732Z","iopub.execute_input":"2024-11-30T04:42:19.571324Z","iopub.status.idle":"2024-11-30T04:42:19.578285Z","shell.execute_reply.started":"2024-11-30T04:42:19.571282Z","shell.execute_reply":"2024-11-30T04:42:19.577483Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here is the metrics after training\nmetrics = {\n            'accuracy': accuracy_score(test_labels[:2000], predictions),\n            'precision': precision_score(test_labels[:2000], predictions),\n            'recall': recall_score(test_labels[:2000], predictions),\n            'f1': f1_score(test_labels[:2000], predictions),\n            'auc_roc': roc_auc_score(test_labels[:2000], scores)\n        }\nmetrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:42:24.872077Z","iopub.execute_input":"2024-11-30T04:42:24.872640Z","iopub.status.idle":"2024-11-30T04:42:24.892812Z","shell.execute_reply.started":"2024-11-30T04:42:24.872603Z","shell.execute_reply":"2024-11-30T04:42:24.892053Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Retrieval Pipeline","metadata":{}},{"cell_type":"code","source":"class RetrievalPipeline:\n    def __init__(self, \n                 bi_encoder_path: str,\n                 cross_encoder_path: str,\n                 faiss_index_path: str,\n                 #embeddings_path: str,\n                 corpus_df_path: str,\n                 top_k: int = 50,\n                 rerank_k: int = 10):\n        \"\"\"\n        Initialize retrieval pipeline\n        Args:\n            bi_encoder_path: Path to bi encoder checkpoint\n            cross_encoder_path: Path to cross encoder checkpoint\n            faiss_index_path: Path to saved FAISS index\n            embeddings_path: Path to saved document embeddings\n            corpus_df_path: Path to corpus CSV file\n            top_k: Number of candidates to retrieve from FAISS\n            rerank_k: Number of final results after reranking\n        \"\"\"\n        # Load models\n        self.bi_encoder = BiEncoder(BiEncoderConfig())\n        self.cross_encoder = CrossEncoderWrapper()\n\n        #Load model\n        self.load_models(bi_encoder_path, cross_encoder_path)\n        \n        # Load FAISS index and embeddings\n        self.index = faiss.read_index(faiss_index_path)\n        #self.document_embeddings = np.load(embeddings_path)\n        \n        # Load corpus for retrieving text\n        self.corpus_df = pd.read_csv(corpus_df_path)\n        \n        # Configuration\n        self.top_k = top_k\n        self.rerank_k = rerank_k\n        \n        # Move models to GPU if available\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.bi_encoder.to(self.device)\n        #self.cross_encoder.model.to(torch.device(\"cuda\"))\n\n    def load_models(self, bi_encoder_path: str, cross_encoder_path: str):\n        bi_encoder_checkpoint = torch.load(bi_encoder_path)\n        question_state_dict = bi_encoder_checkpoint['question_encoder']\n        document_state_dict = bi_encoder_checkpoint['document_encoder']\n\n        question_state_dict = {k.replace('module.', ''): v for k, v in question_state_dict.items()}\n        document_state_dict = {k.replace('module.', ''): v for k, v in document_state_dict.items()}\n                     \n        \n        self.bi_encoder.document_encoder.load_state_dict(\n        document_state_dict)\n        self.bi_encoder.question_encoder.load_state_dict(\n        question_state_dict)\n        #self.bi_encoder.config = bi_encoder_checkpoint['config']\n        \n        self.cross_encoder.load_model(cross_encoder_path)\n\n\n    def retrieve(self, query: str, rerank: bool = True) -> List[Dict[str, Union[str, float, int]]]:\n        \"\"\"\n        Retrieve and rerank documents for a query\n        Args:\n            query: Question string\n        Returns:\n            List of dicts containing retrieved documents with scores and metadata\n        \"\"\"\n        # Clear cache\n        torch.cuda.empty_cache()\n\n        # Stage 1: Bi-encoder retrieval\n        query_embedding = self.bi_encoder.encode_question([query])\n        \n        query_embedding = query_embedding.cpu().numpy()\n        faiss.normalize_L2(query_embedding)\n        \n        # Search FAISS index\n        scores, doc_indices = self.index.search(query_embedding, self.top_k)\n        \n        # Get candidate documents\n        candidates = []\n        for score, doc_idx in zip(scores[0], doc_indices[0]):\n            candidates.append({\n                'text': self.corpus_df.iloc[doc_idx]['text'],\n                'cid': self.corpus_df.iloc[doc_idx]['cid'],\n                'bi_encoder_score': float(score)\n            })\n        if rerank:\n            # Stage 2: Cross-encoder reranking\n            candidate_texts = [c['text'] for c in candidates]\n            #self.bi_encoder.to(torch.device(\"cuda\"))\n            batch_size = 16  # Adjust this based on your GPU memory capacity\n            cross_encoder_scores = []\n\n            for i in range(0, len(candidate_texts), batch_size):\n                batch_texts = candidate_texts[i:i + batch_size]\n                batch_scores = self.cross_encoder.predict(\n                        [query] * len(batch_texts),\n                        batch_texts,\n                        show_progress_bar=False\n                    )\n                cross_encoder_scores.extend(batch_scores)\n            #self.cross_encoder.to(torch.device(\"cpu\"))\n            #Add cross-encoder scores\n            for idx, score in enumerate(cross_encoder_scores):\n                candidates[idx]['cross_encoder_score'] = float(score)\n                \n            candidates.sort(key=lambda x: x['cross_encoder_score'], reverse=True)\n            results = candidates[:self.rerank_k]\n        \n            return results\n\n        else:\n            candidates.sort(key=lambda x: x['bi_encoder_score'], reverse=True)\n            results = candidates[:self.rerank_k]\n\n            return results\n\n    def batch_retrieve(self, \n                      queries: List[str],\n                      batch_size: int = 32) -> List[List[Dict[str, Union[str, float, int]]]]:\n        \"\"\"\n        Batch retrieval for multiple queries\n        \"\"\"\n        all_results = []\n        for i in range(0, len(queries), batch_size):\n            batch = queries[i:i + batch_size]\n            batch_results = [self.retrieve(q) for q in batch]\n            all_results.extend(batch_results)\n        return all_results\n    \n    \ndef evaluate_retrieval(pipeline, test_df, top_k=10, re_ranking=True):\n    \"\"\"\n    Evaluate the retrieval pipeline on a test set using MAP, MRR, NDCG, and Recall@k.\n    \n    Args:\n        pipeline: The retrieval pipeline instance.\n        test_df: DataFrame containing 'question' and 'cid' columns.\n        top_k: Number of top results to consider for evaluation.\n        \n    Returns:\n        A dictionary with MAP, MRR, NDCG, and Recall@k scores.\n    \"\"\"\n    def parse_cid_string(cid_str):\n        \"\"\"Parse a space-separated string of CIDs into a set of integers.\"\"\"\n        return set(map(int, cid_str.strip('[]').split()))\n\n    true_cids = test_df['cid'].apply(parse_cid_string)\n    questions = test_df['question'].tolist()\n    \n    average_precisions = []\n    reciprocal_ranks = []\n    ndcg_scores = []\n    recall_at_k = []\n    \n    for question, true_cid_set in tqdm(zip(questions, true_cids), total=len(questions), desc=\"Evaluating\"):\n        # Retrieve documents\n        results = pipeline.retrieve(question, re_ranking)\n        \n        # Get retrieved cids\n        retrieved_cids = [result['cid'] for result in results[:top_k]]\n        \n        # Calculate Average Precision\n        num_relevant = 0\n        precision_sum = 0.0\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                num_relevant += 1\n                precision_sum += num_relevant / (i + 1)\n        average_precision = precision_sum / len(true_cid_set) if true_cid_set else 0\n        average_precisions.append(average_precision)\n        \n        # Calculate Reciprocal Rank\n        reciprocal_rank = 0.0\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                reciprocal_rank = 1.0 / (i + 1)\n                break\n        reciprocal_ranks.append(reciprocal_rank)\n        \n        # Calculate NDCG\n        dcg = 0.0\n        idcg = sum(1.0 / (i + 1) for i in range(min(len(true_cid_set), top_k)))\n        for i, cid in enumerate(retrieved_cids):\n            if cid in true_cid_set:\n                dcg += 1.0 / (i + 1)\n        ndcg = dcg / idcg if idcg > 0 else 0\n        ndcg_scores.append(ndcg)\n        \n        # Calculate Recall@k\n        relevant_retrieved = len(set(retrieved_cids) & true_cid_set)\n        recall = relevant_retrieved / len(true_cid_set) if true_cid_set else 0\n        recall_at_k.append(recall)\n    \n    # Calculate average metrics\n    map_score = sum(average_precisions) / len(average_precisions)\n    mrr_score = sum(reciprocal_ranks) / len(reciprocal_ranks)\n    avg_ndcg = sum(ndcg_scores) / len(ndcg_scores)\n    avg_recall_at_k = sum(recall_at_k) / len(recall_at_k)\n    \n    return {\n        'MAP': map_score,\n        'MRR': mrr_score,\n        'NDCG': avg_ndcg,\n        'Recall@k': avg_recall_at_k\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:18:15.026843Z","iopub.execute_input":"2024-11-30T05:18:15.027164Z","iopub.status.idle":"2024-11-30T05:18:15.053278Z","shell.execute_reply.started":"2024-11-30T05:18:15.027123Z","shell.execute_reply":"2024-11-30T05:18:15.052405Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"bi_encoder_path = \"/kaggle/input/bi_encoder/pytorch/default/1/bi_encoder.pt\"\ncross_encoder_path = \"/kaggle/input/cross_encoder_new/pytorch/default/1\"\nfaiss_index_path = \"/kaggle/input/bi_encoder_embedding/pytorch/default/1/document_index.faiss\"\ncorpus_df_path = \"/kaggle/input/soict-dataset-2024-segmented/corpus_segmented.csv\"\n\n# Initialize pipeline\npipeline = RetrievalPipeline(\n    bi_encoder_path=bi_encoder_path,\n    cross_encoder_path=cross_encoder_path,\n    faiss_index_path=faiss_index_path,\n    #embeddings_path=\"path/to/embeddings.npy\",\n    corpus_df_path=corpus_df_path,\n    #rerank_k=50\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:18:15.909679Z","iopub.execute_input":"2024-11-30T05:18:15.909944Z","iopub.status.idle":"2024-11-30T05:18:35.111694Z","shell.execute_reply.started":"2024-11-30T05:18:15.909913Z","shell.execute_reply":"2024-11-30T05:18:35.111047Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from underthesea import word_tokenize\n\ndef segment(text: str) -> str:\n    return word_tokenize(text, format=\"text\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:25:34.481002Z","iopub.execute_input":"2024-11-30T05:25:34.481246Z","iopub.status.idle":"2024-11-30T05:25:34.485749Z","shell.execute_reply.started":"2024-11-30T05:25:34.481217Z","shell.execute_reply":"2024-11-30T05:25:34.484912Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Single query\nimport logging\n\nlogging.disable(logging.WARNING)\n\nquery = \"Cơ sở cho thuê nhà trọ có phải đăng ký kinh doanh không?\"\nsegment_query = segment(query)\n\nresults = pipeline.retrieve(segment_query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:03:00.168311Z","iopub.execute_input":"2024-11-30T06:03:00.168590Z","iopub.status.idle":"2024-11-30T06:03:00.877552Z","shell.execute_reply.started":"2024-11-30T06:03:00.168560Z","shell.execute_reply":"2024-11-30T06:03:00.876870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Here is the retrieved document, this is what we'll feed into the LLM (after some \n#processign of course)\nresults","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:03:01.371906Z","iopub.execute_input":"2024-11-30T06:03:01.372466Z","iopub.status.idle":"2024-11-30T06:03:01.378885Z","shell.execute_reply.started":"2024-11-30T06:03:01.372429Z","shell.execute_reply":"2024-11-30T06:03:01.378084Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Let's evaluate the whole pipeline\nfrom sklearn.model_selection import train_test_split\n\ntrain_df = pd.read_csv(\"../input/soict-dataset-2024-segmented/train_segmented.csv\")\ntrain_data, val_data = train_test_split(train_df, test_size=0.1, random_state=42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:28:13.009068Z","iopub.execute_input":"2024-11-30T05:28:13.009335Z","iopub.status.idle":"2024-11-30T05:28:18.275112Z","shell.execute_reply.started":"2024-11-30T05:28:13.009303Z","shell.execute_reply":"2024-11-30T05:28:18.274466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"val_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:28:20.263997Z","iopub.execute_input":"2024-11-30T05:28:20.264345Z","iopub.status.idle":"2024-11-30T05:28:20.275144Z","shell.execute_reply.started":"2024-11-30T05:28:20.264292Z","shell.execute_reply":"2024-11-30T05:28:20.274378Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n# This is the evaluation of our pipeline without reranking, take about 8 minutes to run\nmetrics = evaluate_retrieval(pipeline, val_data, re_ranking=False)\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:28:27.751645Z","iopub.execute_input":"2024-11-30T05:28:27.752454Z","iopub.status.idle":"2024-11-30T05:36:31.841499Z","shell.execute_reply.started":"2024-11-30T05:28:27.752416Z","shell.execute_reply":"2024-11-30T05:36:31.840655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Compare the metric on train and val data\nmetrics_on_train_data = evaluate_retrieval(pipeline, train_data[:1000], re_ranking=False, top_k=50)\nprint(\"metrics on train data \", metrics_on_train_data )\nmetrics_on_val_data = evaluate_retrieval(pipeline, val_data[:1000], re_ranking=False, top_k=50)\nprint(\"metrics on val data \", metrics_on_val_data )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:18:48.822284Z","iopub.execute_input":"2024-11-30T06:18:48.822612Z","iopub.status.idle":"2024-11-30T06:20:09.875438Z","shell.execute_reply.started":"2024-11-30T06:18:48.822581Z","shell.execute_reply":"2024-11-30T06:20:09.874608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%time\n\n# Evaluate on val data with cross-encoder reranking, this take 26 minutes to run!\n# This is why we only use cross-encoder for reranking not for directly retrieval\n# The improvement is impressive:\n# MAP: 0.470 -> 0.559\n# MRR: 0.489 -> 0.576\n# Recall@k: 0.722 -> 0.771\nmetrics = evaluate_retrieval(pipeline, val_data[:1000], top_k=50, re_ranking=True)\nprint(metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T06:22:28.302518Z","iopub.execute_input":"2024-11-30T06:22:28.302770Z","iopub.status.idle":"2024-11-30T06:34:57.201417Z","shell.execute_reply.started":"2024-11-30T06:22:28.302743Z","shell.execute_reply":"2024-11-30T06:34:57.200619Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Helper functions","metadata":{}},{"cell_type":"markdown","source":"### Create embedding for corpus","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# import torch\n# import faiss\n# import numpy as np\n# from tqdm import tqdm\n# from typing import Tuple\n\n# def build_faiss_index(\n#     corpus_path: str,\n#     model: BiEncoder,\n#     batch_size: int = 32,\n#     index_path: str = \"document_index.faiss\",\n#     embeddings_path: str = \"document_embeddings.npy\",\n#     nlist: int = 1000,  # Number of clusters/cells\n#     nprobe: int = 100,  # Number of cells to visit during search\n# ) -> faiss.IndexIVFFlat:\n    # \"\"\"\n    # Build optimized FAISS index from corpus documents\n    \n    # Args:\n    #     corpus_path: Path to corpus CSV file\n    #     model: Trained BiEncoder model\n    #     batch_size: Batch size for encoding\n    #     index_path: Where to save the FAISS index\n    #     nlist: Number of Voronoi cells (clusters)\n    #     nprobe: Number of nearest cells to search\n    # \"\"\"\n    # print(\"Reading corpus...\")\n    # df = pd.read_csv(corpus_path)\n    # documents = df['text'].tolist()\n    \n    # # Prepare model\n    # model.eval()\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # model.to(device)\n\n    # # Initialize lists to store batched embeddings\n    # all_embeddings = []\n    \n    # # Process in batches to handle memory efficiently\n    # print(\"Encoding documents...\")\n    \n    # progress_bar = tqdm(total=int(len(documents)/batch_size),\n    #                         desc=\"Encoding documents\",\n    #                         ncols=80,\n    #                         position=0,  # Force position to 0\n    #                         leave=True)  # Keep final result visible\n    \n    # with torch.no_grad():\n    #     for i in range(0, len(documents), batch_size):\n    #         batch = documents[i:i + batch_size]\n    #         embeddings = model.encode_document(batch, disable_progress_bar= True )\n    #         all_embeddings.append(embeddings.cpu())\n    #         progress_bar.update(1)\n    \n    # # Concatenate all embeddings\n    # document_embeddings = torch.cat(all_embeddings, dim=0).numpy()\n    # dimension = document_embeddings.shape[1]\n\n    # # Normalize embeddings\n    # faiss.normalize_L2(document_embeddings)\n\n    # # Create GPU resource\n    # res = faiss.StandardGpuResources()\n    \n    # # Configure index\n    # quantizer = faiss.IndexFlatIP(dimension)\n    # index = faiss.IndexIVFFlat(quantizer, dimension, nlist, faiss.METRIC_INNER_PRODUCT)\n    \n    # # Transfer to GPU for training and adding vectors\n    # gpu_index = faiss.index_cpu_to_gpu(res, 0, index)\n    \n    # # Train index\n    # print(\"Training index...\")\n    # gpu_index.train(document_embeddings)\n    \n    # # Add vectors to index\n    # print(\"Adding vectors to index...\")\n    # gpu_index.add(document_embeddings)\n    \n    # # Set number of cells to probe during search\n    # gpu_index.nprobe = nprobe\n    \n    # # Transfer back to CPU for saving\n    # index = faiss.index_gpu_to_cpu(gpu_index)\n    \n    # # Save index\n    # print(\"Saving index...\")\n    # faiss.write_index(index, index_path)\n    \n    # #  Save Embeddings\n    # print(\"Saving embeddings...\")\n    # np.save(embeddings_path, document_embeddings)\n    \n    # print(f\"Index built with {index.ntotal} vectors of dimension {dimension}\")\n    # print(f\"Number of clusters: {nlist}, nprobe: {nprobe}\")\n    # return index, document_embeddings\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:55:26.479467Z","iopub.execute_input":"2024-11-30T04:55:26.480161Z","iopub.status.idle":"2024-11-30T04:55:26.490129Z","shell.execute_reply.started":"2024-11-30T04:55:26.480127Z","shell.execute_reply":"2024-11-30T04:55:26.489317Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# config = BiEncoderConfig(\n#     max_length=256,\n#     batch_size=16,\n#     learning_rate=1e-5,\n#     num_epochs=2,\n#     temperature=0.05\n# )\n\n# trainer = BiEncoderTrainer(config)\n# trainer.load_model(\"/kaggle/input/bi_encoder/pytorch/default/1/bi_encoder.pt\")\n# model = trainer.get_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-30T05:08:14.532308Z","iopub.execute_input":"2024-11-30T05:08:14.532940Z","iopub.status.idle":"2024-11-30T05:08:16.073324Z","shell.execute_reply.started":"2024-11-30T05:08:14.532907Z","shell.execute_reply":"2024-11-30T05:08:16.072626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# index, document_embeddings = build_faiss_index(\n#                 corpus_path= \"../input/soict-dataset-2024-segmented/corpus_segmented.csv\",\n#                 model = model\n#             )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T14:21:37.177437Z","iopub.execute_input":"2024-11-29T14:21:37.178162Z","iopub.status.idle":"2024-11-29T15:00:25.029480Z","shell.execute_reply.started":"2024-11-29T14:21:37.178112Z","shell.execute_reply":"2024-11-29T15:00:25.028205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def prepare_cross_encoder_data(\n#     train_df: pd.DataFrame,\n#     bi_encoder_model: BiEncoder,\n#     corpus_df: pd.DataFrame,\n#     num_hard_negatives: int = 3,\n#     num_random_negatives: int = 1,\n#     batch_size: int = 32,\n#     embeddings_path: str = \"document_embeddings.npy\"\n# ) -> pd.DataFrame:\n#     \"\"\"\n#     Prepare training data for cross-encoder with multiple types of negatives using pre-computed document embeddings\n    \n#     Args:\n    #     train_df: Training dataframe with questions and ground truths\n    #     bi_encoder_model: Trained bi-encoder model for finding hard negatives\n    #     corpus_df: Full corpus dataframe\n    #     num_hard_negatives: Number of hard negatives per question\n    #     num_random_negatives: Number of random negatives per question\n    #     batch_size: Batch size for bi-encoder inference\n    #     embeddings_path: Path to saved document embeddings\n    # \"\"\"\n    # training_pairs = []\n    \n    # # Create corpus lookup\n    # corpus_lookup = dict(zip(corpus_df['cid'], corpus_df['text']))\n    \n    \n    # def parse_cids(cid_str: str) -> List[int]:\n    #     \"\"\"Parse space-separated CIDs\"\"\"\n    #     if isinstance(cid_str, str):\n    #         # Remove brackets and split by whitespace\n    #         return [int(cid) for cid in cid_str.strip('[]').split()]\n    #     return cid_str\n    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # # Load pre-computed document embeddings\n    # print(\"Loading pre-computed document embeddings...\")\n    # d_embeddings = torch.from_numpy(np.load(embeddings_path)).to(device)\n\n    # # Process questions in batches with accurate progress tracking\n    # training_pairs = []\n    # total_questions = len(train_df)\n    \n    # with tqdm(total=total_questions, desc=\"Processing questions\") as pbar:\n    #     for start_idx in range(0, len(train_df), batch_size):\n    #         batch_df = train_df.iloc[start_idx:start_idx + batch_size]\n            \n    #         # Get question embeddings for the batch\n    #         questions = batch_df['question'].tolist()\n    #         q_embeddings = bi_encoder_model.encode_question(questions)\n            \n    #         # Compute similarities\n    #         similarities = torch.matmul(q_embeddings, d_embeddings.t())\n            \n            # # Process each question in the batch\n            # for batch_idx, (_, row) in enumerate(batch_df.iterrows()):\n            #     question = row['question']\n                \n            #     # Parse context and CIDs\n            #     correct_docs = row['context']\n            #     correct_cids = parse_cids(row['cid'])\n                \n            #     # Add positive pairs\n            #     for doc, cid in zip([correct_docs] * len(correct_cids), correct_cids):\n            #         training_pairs.append({\n            #             'question': question,\n            #             'document': doc,\n            #             'label': 1,\n            #             'cid': int(cid)\n            # #         })\n                \n            #     # Get hard negatives\n            #     q_sim = similarities[batch_idx]\n            #     _, candidate_indices = q_sim.topk(num_hard_negatives + len(correct_cids))\n                \n            #     # Filter out correct documents\n            #     hard_negative_indices = [\n            #         idx.item() for idx in candidate_indices \n            #         if corpus_df.iloc[int(idx.item())]['cid'] not in correct_cids\n            #     ][:num_hard_negatives]\n                \n                # # Add hard negative pairs\n                # for neg_idx in hard_negative_indices:\n                #     neg_doc = corpus_df.iloc[int(neg_idx)]\n                #     training_pairs.append({\n                #         'question': question,\n                #         'document': neg_doc['text'],\n                #         'label': 0,\n                #         'cid': neg_doc['cid']\n                #     })\n                \n                # # Add random negatives\n                # random_negative_cids = np.random.choice(\n                #     [cid for cid in corpus_df['cid'] if cid not in correct_cids],\n                #     size=num_random_negatives,\n                #     replace=False\n                # )\n                \n    #             for neg_cid in random_negative_cids:\n    #                 training_pairs.append({\n    #                     'question': question,\n    #                     'document': corpus_lookup[neg_cid],\n    #                     'label': 0,\n    #                     'cid': neg_cid\n    #                 })\n                \n    #             # Update progress bar for each processed question\n    #             pbar.update(1)\n    # return pd.DataFrame(training_pairs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:08:02.635969Z","iopub.execute_input":"2024-11-29T16:08:02.636407Z","iopub.status.idle":"2024-11-29T16:08:02.654809Z","shell.execute_reply.started":"2024-11-29T16:08:02.636375Z","shell.execute_reply":"2024-11-29T16:08:02.653539Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Load data orginal dataset to create dataset for training cross encoder\n# corpus_path = \"../input/soict-dataset-2024-segmented/corpus_segmented.csv\"\n# train_path = \"../input/soict-dataset-2024-segmented/train_segmented.csv\"\n# corpus_df = pd.read_csv(corpus_path)\n# train_df = pd.read_csv(train_path)\n# corpus_df.shape, train_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:01:16.722444Z","iopub.execute_input":"2024-11-29T16:01:16.723172Z","iopub.status.idle":"2024-11-29T16:01:28.116951Z","shell.execute_reply.started":"2024-11-29T16:01:16.723136Z","shell.execute_reply":"2024-11-29T16:01:28.116030Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Split into train and validation\n# val_size = 0.1\n# train_data, val_data = train_test_split(train_df, test_size=val_size, random_state=42)\n# train_data.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:01:28.118269Z","iopub.execute_input":"2024-11-29T16:01:28.118555Z","iopub.status.idle":"2024-11-29T16:01:28.149184Z","shell.execute_reply.started":"2024-11-29T16:01:28.118528Z","shell.execute_reply":"2024-11-29T16:01:28.148205Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Prepare training data with negatives\n# cross_encoder_data = prepare_cross_encoder_data(\n#     train_data[:50000],\n#     model,\n#     corpus_df,\n#     num_hard_negatives=3,\n#     num_random_negatives=1,\n#     embeddings_path = \"/kaggle/input/bi_encoder_embedding/pytorch/default/1/document_embeddings.npy\"\n# )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T16:08:10.614329Z","iopub.execute_input":"2024-11-29T16:08:10.615380Z","iopub.status.idle":"2024-11-29T16:58:35.263020Z","shell.execute_reply.started":"2024-11-29T16:08:10.615344Z","shell.execute_reply":"2024-11-29T16:58:35.262212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#cross_encoder_data.to_csv('train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T17:00:12.961002Z","iopub.execute_input":"2024-11-29T17:00:12.961402Z","iopub.status.idle":"2024-11-29T17:00:22.839787Z","shell.execute_reply.started":"2024-11-29T17:00:12.961370Z","shell.execute_reply":"2024-11-29T17:00:22.838898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}